import akshare as ak
import json
import os
import time
import pandas as pd
from datetime import datetime
import hashlib
import pytz
from bs4 import BeautifulSoup

# --- Selenium æ¨¡å— (æ¨¡æ‹Ÿæµè§ˆå™¨) ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- é…ç½® ---
DATA_DIR = "data_news"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

def get_beijing_time():
    return datetime.now(pytz.timezone('Asia/Shanghai'))

def get_today_str():
    return get_beijing_time().strftime("%Y-%m-%d")

def generate_news_id(item):
    raw = f"{item.get('time','')}{item.get('title','')}"
    return hashlib.md5(raw.encode('utf-8')).hexdigest()

def clean_time_str(t_str):
    if not t_str: return ""
    try:
        # å°è¯•è§£æå¸¸è§æ ¼å¼
        if len(str(t_str)) == 10: 
             return datetime.fromtimestamp(int(t_str)).strftime("%Y-%m-%d %H:%M:%S")
        if len(str(t_str)) > 19:
            return str(t_str)[:19]
        return str(t_str)
    except:
        return str(t_str)

# ==========================================
# 1. ä¸œè´¢æŠ“å– (ä½¿ç”¨ Akshare API)
# ==========================================
def fetch_eastmoney():
    items = []
    try:
        print("   - [API] æ­£åœ¨æŠ“å–: ä¸œæ–¹è´¢å¯Œ (EastMoney)...")
        # å¼ºåˆ¶æ›´æ–°ä¸€ä¸‹æ¥å£ï¼Œé˜²æ­¢æŠ¥é”™
        df_em = ak.stock_telegraph_em()
        if df_em is not None and not df_em.empty:
            for _, row in df_em.iterrows():
                title = str(row.get('title', '')).strip()
                content = str(row.get('content', '')).strip()
                public_time = clean_time_str(row.get('public_time', ''))
                
                if not title or len(title) < 2: continue
                items.append({
                    "time": public_time,
                    "title": title,
                    "content": content,
                    "source": "EastMoney"
                })
    except Exception as e:
        print(f"   âŒ ä¸œè´¢æŠ“å–å¤±è´¥: {e}")
    return items

# ==========================================
# 2. è´¢è”ç¤¾æŠ“å– (ä½¿ç”¨ Selenium æ¨¡æ‹Ÿæµè§ˆå™¨)
# ==========================================
def fetch_cls_selenium():
    items = []
    driver = None
    try:
        print("   - [Browser] æ­£åœ¨å¯åŠ¨ Chrome æŠ“å–: è´¢è”ç¤¾ (CLS)...")
        
        # é…ç½®æ— å¤´æµè§ˆå™¨ (Headless Chrome)
        chrome_options = Options()
        chrome_options.add_argument("--headless") # æ— ç•Œé¢æ¨¡å¼
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        # ä¼ªè£… User-Agent
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

        # è‡ªåŠ¨å®‰è£…å¹¶å¯åŠ¨ Driver
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # è®¾ç½®è¶…æ—¶
        driver.set_page_load_timeout(30)
        
        # è®¿é—®è´¢è”ç¤¾ç”µæŠ¥é¡µé¢
        url = "https://www.cls.cn/telegraph"
        driver.get(url)
        
        # ç­‰å¾…å†…å®¹åŠ è½½ (ç­‰å¾…åˆ—è¡¨å‡ºç°)
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "telegraph-list"))
            )
        except:
            print("   âš ï¸ ç­‰å¾…ç½‘é¡µåŠ è½½è¶…æ—¶ï¼Œå°è¯•ç›´æ¥è§£æ...")

        # æ»šåŠ¨ä¸€ä¸‹å±å¹•è§¦å‘æ‡’åŠ è½½
        driver.execute_script("window.scrollTo(0, 1000);")
        time.sleep(2) 

        # è·å–é¡µé¢ HTML
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        # è§£ææ•°æ® (æ ¹æ®è´¢è”ç¤¾ç½‘é¡µç»“æ„)
        # é€šå¸¸æ˜¯ä¸€ä¸ª class="telegraph-list" çš„åˆ—è¡¨
        # æ¯ä¸€é¡¹å¯èƒ½æœ‰ class="telegraph-content-box" ç­‰
        
        # å¯»æ‰¾æ‰€æœ‰çš„æ—¶é—´çº¿èŠ‚ç‚¹ (è¿™éœ€è¦æ ¹æ® cls å®é™… html ç»“æ„è°ƒæ•´ï¼Œä»¥ä¸‹æ˜¯é€šç”¨æŠ“å–é€»è¾‘)
        # ç›®å‰ CLS ç»“æ„é€šå¸¸æ˜¯: div.telegraph-list -> div.telegraph-list-item
        nodes = soup.find_all("div", class_="telegraph-list-item")
        
        if not nodes:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•æ‰¾æ‰€æœ‰å¸¦æ—¶é—´æˆ³æ ·å¼çš„æ–‡æœ¬
            nodes = soup.select("div.telegraph-content-box")

        print(f"   - æ•è·åˆ° {len(nodes)} ä¸ªç½‘é¡µèŠ‚ç‚¹")

        current_date_prefix = get_beijing_time().strftime("%Y-%m-%d")

        for node in nodes:
            try:
                # æå–æ—¶é—´ (é€šå¸¸åœ¨ span ä¸­)
                time_span = node.find("span", class_="telegraph-time")
                time_str = time_span.get_text().strip() if time_span else ""
                
                # è¡¥å…¨æ—¥æœŸ (ç½‘é¡µé€šå¸¸åªæ˜¾ç¤º 14:30)
                if len(time_str) <= 5 and ":" in time_str:
                    full_time = f"{current_date_prefix} {time_str}:00"
                else:
                    full_time = time_str

                # æå–å†…å®¹
                content_div = node.find("div", class_="telegraph-content")
                if not content_div:
                    # å°è¯•å¤‡ç”¨ç»“æ„
                    content_div = node.find("div", class_="telegraph-detail")
                
                content_text = content_div.get_text().strip() if content_div else ""
                
                # è´¢è”ç¤¾ç”µæŠ¥é€šå¸¸æ²¡æœ‰ç‹¬ç«‹æ ‡é¢˜ï¼Œå†…å®¹ç¬¬ä¸€å¥å³æ ‡é¢˜
                if content_text:
                    title = content_text[:40] + "..." if len(content_text) > 40 else content_text
                    
                    items.append({
                        "time": full_time,
                        "title": title,
                        "content": content_text,
                        "source": "CLS"
                    })
            except: continue

    except Exception as e:
        print(f"   âŒ è´¢è”ç¤¾(Selenium)æŠ“å–å¤±è´¥: {e}")
    finally:
        if driver:
            driver.quit()
    
    return items

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def fetch_and_save_news():
    today_date = get_today_str()
    print(f"ğŸ“¡ [NewsLoader] å¯åŠ¨æ··åˆæŠ“å– (Akshare + Selenium) - {today_date}...")
    
    all_news_items = []

    # 1. ä¸œè´¢
    em_items = fetch_eastmoney()
    all_news_items.extend(em_items)

    # 2. è´¢è”ç¤¾ (æµè§ˆå™¨æ¨¡å¼)
    cls_items = fetch_cls_selenium()
    all_news_items.extend(cls_items)

    # 3. å…¥åº“
    if not all_news_items:
        print("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
        return

    today_file = os.path.join(DATA_DIR, f"news_{today_date}.jsonl")
    existing_ids = set()
    
    if os.path.exists(today_file):
        with open(today_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    saved_item = json.loads(line)
                    if 'id' in saved_item:
                        existing_ids.add(saved_item['id'])
                except: pass

    new_count = 0
    # ç®€å•æŒ‰æ—¶é—´å­—ç¬¦ä¸²å€’åº
    all_news_items.sort(key=lambda x: x['time'], reverse=True)

    with open(today_file, 'a', encoding='utf-8') as f:
        for item in all_news_items:
            item_id = generate_news_id(item)
            item['id'] = item_id
            
            if item_id not in existing_ids:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
                existing_ids.add(item_id)
                new_count += 1
    
    print(f"âœ… å…¥åº“å®Œæˆ: æ–°å¢ {new_count} æ¡ (EM:{len(em_items)} | CLS:{len(cls_items)})")

if __name__ == "__main__":
    fetch_and_save_news()
