import pandas as pd
import time
import random
import os
import yaml
import logging
import gc
import sys
from datetime import datetime, time as dt_time
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass

# æ£€æŸ¥ä¾èµ–
try:
    from curl_cffi import requests as cffi_requests
    from curl_cffi.requests.exceptions import RequestException, ProxyError, Timeout
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install curl_cffi pandas pyyaml")
    sys.exit(1)

# ===================== é…ç½® =====================

# ğŸŸ¢ å·²ç¡¬ç¼–ç ä½ çš„ ScraperAPI Key
SCRAPERAPI_KEY = "051bfb47887b7b5c254b7f78d39e2c4f"

# å¦‚æœ ScraperAPI é¢åº¦è€—å°½(403é”™è¯¯)æˆ–å¤±è´¥ï¼Œæ˜¯å¦å…è®¸è‡ªåŠ¨é™çº§ä¸ºæœ¬æœºç›´è¿ï¼Ÿ
# å»ºè®®ä¸º Trueï¼Œå› ä¸ºä¸œè´¢å¯¹æœ¬æœºå°‘é‡æŠ“å–é€šå¸¸æ˜¯æ”¾è¡Œçš„
ALLOW_DIRECT_FALLBACK = True 

def get_beijing_time():
    from datetime import timezone, timedelta
    return datetime.now(timezone(timedelta(hours=8)))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===================== DataFetcher =====================
class DataFetcher:
    UNIFIED_COLUMNS = ['date', 'open', 'high', 'low', 'close', 'volume',
                       'amount', 'amplitude', 'pct_change', 'change', 'turnover_rate', 'fetch_time']
    
    def __init__(self):
        self.DATA_DIR = "data_cache"
        os.makedirs(self.DATA_DIR, exist_ok=True)
        
        self.spot_data_cache: Optional[pd.DataFrame] = None
        self.spot_data_date: Optional[str] = None
        self.session: Optional[cffi_requests.Session] = None
        
        # ç»Ÿè®¡
        self.total_funds = 0
        self.success_count = 0
        
    def _get_scraperapi_proxy(self) -> str:
        """æ„é€  ScraperAPI ä»£ç†å­—ç¬¦ä¸²"""
        return f"http://scraperapi:{SCRAPERAPI_KEY}@proxy-server.scraperapi.com:8001"

    def _create_session(self, use_proxy: bool = True):
        """åˆ›å»º Session"""
        self._close_session()
        
        try:
            if use_proxy and SCRAPERAPI_KEY:
                proxy_url = self._get_scraperapi_proxy()
                self.session = cffi_requests.Session(
                    impersonate="chrome120",
                    proxies={"http": proxy_url, "https": proxy_url},
                    timeout=60,  # ScraperAPI éœ€è¦æ—¶é—´å¯»æ‰¾èŠ‚ç‚¹
                    verify=False 
                )
                logger.info(f"ğŸŒ ä½¿ç”¨ ScraperAPI ä»£ç†é€šé“")
            else:
                self.session = cffi_requests.Session(
                    impersonate="chrome120",
                    timeout=30
                )
                logger.info(f"ğŸ”Œ ä½¿ç”¨ç›´è¿æ¨¡å¼ (æ— ä»£ç†)")
                
        except Exception as e:
            logger.error(f"âŒ åˆ›å»º session å¤±è´¥: {e}")
            raise

    def _close_session(self):
        if self.session:
            try:
                self.session.close()
            except:
                pass
            self.session = None
            gc.collect()

    def _safe_request(self, url: str, params: dict, headers: dict, max_retries: int = 3) -> Optional[dict]:
        """
        è¯·æ±‚é€»è¾‘ï¼š
        1. é»˜è®¤å°è¯•ä½¿ç”¨ ScraperAPI
        2. å¦‚æœ ScraperAPI å¤±è´¥ (403/Timeout) ä¸”å…è®¸ Fallbackï¼Œå°è¯•ç›´è¿
        """
        
        # ç¡®ä¿ session å­˜åœ¨ï¼Œé»˜è®¤ä¸ºä»£ç†æ¨¡å¼
        if self.session is None:
            self._create_session(use_proxy=True)

        for attempt in range(max_retries):
            try:
                if not self.session:
                    raise Exception("Session Lost")
                
                # å‘èµ·è¯·æ±‚
                r = self.session.get(url, params=params, headers=headers)
                
                # ScraperAPI ç‰¹æœ‰é”™è¯¯ç å¤„ç†
                if r.status_code == 403:
                    logger.warning("âš ï¸ ScraperAPI è¿”å› 403 (å¯èƒ½ Key æ— æ•ˆæˆ–é¢åº¦è€—å°½)")
                    if ALLOW_DIRECT_FALLBACK:
                         logger.info("ğŸ”„ é™çº§ä¸ºç›´è¿é‡è¯•...")
                         self._create_session(use_proxy=False)
                         # ç«‹å³é‡è¯•
                         try:
                             r = self.session.get(url, params=params, headers=headers)
                             r.raise_for_status()
                             return r.json()
                         except Exception as e:
                             logger.error(f"âŒ ç›´è¿é‡è¯•ä¹Ÿå¤±è´¥: {e}")
                             return None
                    else:
                        return None
                        
                r.raise_for_status()
                return r.json()
                
            except (ProxyError, Timeout, RequestException) as e:
                logger.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ ({attempt+1}/{max_retries}): {str(e)[:100]}")
                time.sleep(2) 
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ä¸”å…è®¸ç›´è¿ï¼Œå°è¯•æœ€åä¸€æ¬¡ç›´è¿
                if attempt == max_retries - 1 and ALLOW_DIRECT_FALLBACK:
                     logger.info("ğŸ”„ æœ€ç»ˆå°è¯•ï¼šåˆ‡æ¢åˆ°ç›´è¿æ¨¡å¼")
                     self._create_session(use_proxy=False)
                     try:
                         r = self.session.get(url, params=params, headers=headers)
                         r.raise_for_status()
                         return r.json()
                     except:
                         pass
        
        logger.error("âŒ æ‰€æœ‰å°è¯•å‡å¤±è´¥")
        return None

    def fetch_all_etfs(self) -> Optional[pd.DataFrame]:
        """è·å–å…¨å¸‚åœº ETF æ•°æ®"""
        url = "https://push2.eastmoney.com/api/qt/clist/get"
        
        all_data = []
        page = 1
        consecutive_errors = 0
        
        logger.info("ğŸ“¡ å¼€å§‹è·å– ETF å…¨é‡åˆ—è¡¨ (Via ScraperAPI)...")
        
        while page <= 200 and consecutive_errors < 3:
            if page % 10 == 0:
                logger.info(f"ğŸ“„ è·å–ç¬¬ {page} é¡µ...")
            
            params = {
                "pn": str(page),
                "pz": "100",
                "po": "1",
                "np": "1",
                "ut": "bd1d9ddb04089700cf9c27f6f7426281",
                "fltt": "2",
                "invt": "2",
                "fid": "f3",
                "fs": "b:MK0021,b:MK0022,b:MK0023,b:MK0024",
                "fields": "f12,f14,f2,f3,f4,f5,f6,f7,f8,f15,f16,f17,f18",
                "_": str(int(time.time() * 1000))
            }
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Referer": "http://quote.eastmoney.com/",
            }
            
            data = self._safe_request(url, params, headers, max_retries=3)
            
            if not data or data.get('rc') != 0 or 'data' not in data or 'diff' not in data['data']:
                consecutive_errors += 1
                logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæ•°æ®å¼‚å¸¸ (è¿ç»­é”™è¯¯ {consecutive_errors}/3)")
                if consecutive_errors >= 3:
                    break
                continue
            
            consecutive_errors = 0
            items = data['data']['diff']
            
            if not items:
                break
                
            all_data.extend(items)
            logger.info(f"   âœ… æœ¬é¡µ {len(items)} æ¡")
            
            if len(items) < 100:
                break
            
            page += 1
            time.sleep(0.5) 
        
        self._close_session()
        
        if not all_data:
            return None
        
        # å¤„ç†æ•°æ®
        df = pd.DataFrame(all_data)
        rename_map = {
            'f12': 'code', 'f14': 'name', 'f2': 'close', 'f3': 'pct_change',
            'f4': 'change', 'f5': 'volume', 'f6': 'amount', 'f7': 'amplitude',
            'f8': 'turnover_rate', 'f17': 'open', 'f15': 'high', 'f16': 'low',
        }
        df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns}, inplace=True)
        # å…¼å®¹æ€§å¤„ç†ï¼šé˜²æ­¢ç©ºæ•°æ®æŠ¥é”™
        if 'code' in df.columns:
            df['code'] = df['code'].astype(str).str.strip().str.lower().str.replace(r'^(sh|sz)', '', regex=True)
            df = df.drop_duplicates(subset=['code'], keep='first')
            logger.info(f"âœ… å…±è·å– {len(df)} åª ETF")
            return df.set_index('code')
        else:
            return None

    def init_spot_data(self) -> bool:
        today = get_beijing_time().strftime("%Y-%m-%d")
        
        if self.spot_data_cache is not None and self.spot_data_date == today:
            return True
        
        df = self.fetch_all_etfs()
        if df is not None and not df.empty:
            self.spot_data_cache = df
            self.spot_data_date = today
            return True
        return False

    def update_single(self, fund_code: str) -> bool:
        if self.spot_data_cache is None:
            if not self.init_spot_data():
                return False
        
        code = str(fund_code).strip().lower().replace('sh', '').replace('sz', '')
        
        if code not in self.spot_data_cache.index:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {fund_code}")
            return False
        
        try:
            row = self.spot_data_cache.loc[code]
            today = pd.Timestamp(get_beijing_time().date())
            
            def to_float(x):
                try: return float(x) if x and x != '-' else 0.0
                except: return 0.0
            
            new_data = {
                'date': today,
                'open': to_float(row.get('open')),
                'high': to_float(row.get('high')),
                'low': to_float(row.get('low')),
                'close': to_float(row.get('close')),
                'volume': to_float(row.get('volume')),
                'amount': to_float(row.get('amount')),
                'amplitude': to_float(row.get('amplitude')),
                'pct_change': to_float(row.get('pct_change')),
                'change': to_float(row.get('change')),
                'turnover_rate': to_float(row.get('turnover_rate')),
                'fetch_time': get_beijing_time().strftime("%Y-%m-%d %H:%M:%S"),
                'source': 'eastmoney_spot'
            }
            
            df_new = pd.DataFrame([new_data])
            df_new.set_index('date', inplace=True)
            
            path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            if os.path.exists(path):
                try:
                    df_old = pd.read_csv(path, index_col='date', parse_dates=['date'])
                    if today in df_old.index:
                        df_old.update(df_new)
                        df_final = df_old
                    else:
                        df_final = pd.concat([df_old, df_new])
                    df_final = df_final[~df_final.index.duplicated(keep='last')].sort_index()
                except:
                    df_final = df_new
            else:
                df_final = df_new
            
            df_final = df_final.reindex(columns=self.UNIFIED_COLUMNS)
            df_final.to_csv(path)
            return True
            
        except Exception as e:
            logger.error(f"âŒ {fund_code} å¤„ç†å¤±è´¥: {e}")
            return False

    def run(self, funds: List[dict]):
        self.total_funds = len(funds)
        self.success_count = 0
        
        # æµ‹è¯•ç½‘ç»œ
        logger.info("ğŸ” æ­£åœ¨è¿æ¥ ScraperAPI ...")
        # æµ‹è¯•ä¸€ä¸ªç®€å•çš„ API ç¡®ä¿ä»£ç†é€šç•…
        test = self._safe_request("https://push2.eastmoney.com/api/qt/clist/get", 
                                  {"pn":"1","pz":"1","fs":"b:MK0021"}, {}, max_retries=2)
        if not test:
            logger.error("âŒ æ— æ³•è¿æ¥ (è¯·æ£€æŸ¥ ScraperAPI é¢åº¦ æˆ– ç½‘ç»œ)")
            return 0

        if not self.init_spot_data():
            return 0
        
        for i, fund in enumerate(funds, 1):
            code = str(fund.get('code', '')).strip()
            if not code: continue
            
            if self.update_single(code):
                self.success_count += 1
            
            if i % 50 == 0:
                 logger.info(f"ğŸ“Š è¿›åº¦: {i}/{self.total_funds}, æˆåŠŸ: {self.success_count}")
        
        return self.success_count

# ===================== ä¸»å…¥å£ =====================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ DataFetcher V23.1 (ScraperAPI Hardcoded)")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿé…ç½® (å¦‚æœæ²¡æ‰¾åˆ°configæ–‡ä»¶)
    funds = []
    if os.path.exists('config.yaml'):
        with open('config.yaml', 'r', encoding='utf-8') as f:
            cfg = yaml.safe_load(f)
            funds = cfg.get('funds', [])
    else:
        logger.warning("âš ï¸ ä½¿ç”¨æµ‹è¯•æ•°æ® (config.yaml æœªæ‰¾åˆ°)")
        funds = [{'code': '510300', 'name': 'æ²ªæ·±300ETF'}, {'code': '510050', 'name': 'ä¸Šè¯50ETF'}]
    
    if not funds:
        print("âŒ åŸºé‡‘åˆ—è¡¨ä¸ºç©º")
        sys.exit(1)
    
    fetcher = DataFetcher()
    success = fetcher.run(funds)
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ å®Œæˆ: {success}/{len(funds)}")
    print(f"{'=' * 60}")
    
    sys.exit(0 if success > 0 else 1)
