import pandas as pd
import time
import random
import os
import yaml
import logging
import gc
import json
from datetime import datetime, time as dt_time
from typing import Optional, Tuple

# [å…³é”®ä¾èµ–] å¼•å…¥ curl_cffi æ¨¡æ‹Ÿæµè§ˆå™¨æŒ‡çº¹
try:
    from curl_cffi import requests as cffi_requests
    from curl_cffi.requests.exceptions import RequestException
except ImportError:
    raise ImportError("è¯·å…ˆå®‰è£… curl_cffi: pip install curl_cffi>=0.5.10")

# ===================== å·¥å…·å‡½æ•° =====================
def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    from datetime import timezone, timedelta
    return datetime.now(timezone(timedelta(hours=8)))

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def retry(retries: int = 3, delay: float = 5.0):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"âš ï¸ [Retry {i+1}/{retries}] {func.__name__} å¤±è´¥: {e}")
                    if i < retries - 1:
                        time.sleep(delay * (i + 1))  # é€’å¢å»¶è¿Ÿ
            logger.error(f"âŒ {func.__name__} é‡è¯•è€—å°½")
            return None
        return wrapper
    return decorator

# ===================== DataFetcher ç±» =====================
class DataFetcher:
    UNIFIED_COLUMNS = [
        'date', 'open', 'high', 'low', 'close', 'volume',
        'amount', 'amplitude', 'pct_change', 'change', 'turnover_rate',
        'fetch_time'
    ]
    
    def __init__(self):
        self.DATA_DIR = "data_cache"
        os.makedirs(self.DATA_DIR, exist_ok=True)
        
        # ç¼“å­˜å…¨å¸‚åœºæ•°æ®
        self.spot_data_cache: Optional[pd.DataFrame] = None
        self.spot_data_date: Optional[str] = None
        
        # åˆ›å»º session å¤ç”¨ï¼ˆcurl_cffi æ”¯æŒï¼‰
        self.session = cffi_requests.Session(impersonate="chrome120")

    def __del__(self):
        """æ¸…ç† session"""
        if hasattr(self, 'session'):
            try:
                self.session.close()
            except:
                pass

    def _standardize_dataframe(self, df: pd.DataFrame, source_name: str) -> pd.DataFrame:
        """æ ‡å‡†åŒ– DataFrame æ ¼å¼"""
        if df is None or df.empty:
            return df
        
        df = df.copy()
        
        # ç¡®ä¿æ‰€æœ‰ç»Ÿä¸€å­—æ®µéƒ½å­˜åœ¨
        for col in self.UNIFIED_COLUMNS:
            if col not in df.columns:
                df[col] = pd.NA
        
        # æŒ‰ç»Ÿä¸€é¡ºåºæ’åˆ—
        df = df[self.UNIFIED_COLUMNS]
        
        # å¼ºåˆ¶è½¬ä¸ºæ•°å­—ç±»å‹
        numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 
                       'amplitude', 'pct_change', 'change', 'turnover_rate']
        for col in numeric_cols:
            if col in df.columns:
                df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')
        
        return df

    @retry(retries=3, delay=5)
    def _fetch_eastmoney_raw_spot(self) -> Optional[pd.DataFrame]:
        """
        [æ ¸å¿ƒé»‘ç§‘æŠ€] ä½¿ç”¨ curl_cffi ç›´æ¥è¯·æ±‚ä¸œè´¢åŸå§‹æ¥å£
        ç»•è¿‡ Akshareï¼Œç›´æ¥æ¨¡æ‹Ÿ Chrome 120 è·å–å…¨å¸‚åœº ETF æ•°æ®
        """
        # [ä¿®å¤] URL å»é™¤ç©ºæ ¼
        url = "https://push2.eastmoney.com/api/qt/clist/get"
        
        # ä¸œè´¢ ETF æ¿å—å‚æ•°
        params = {
            "pn": "1",
            "pz": "5000",  # ä¸€æ¬¡æ‹‰å– 5000 åªï¼Œè¦†ç›–æ‰€æœ‰ ETF
            "po": "1",
            "np": "1",
            "ut": "bd1d9ddb04089700cf9c27f6f7426281",
            "fltt": "2",
            "invt": "2",
            "fid": "f3",
            "fs": "b:MK0021,b:MK0022,b:MK0023,b:MK0024",  # æ¶µç›–æ‰€æœ‰åœºå†…åŸºé‡‘
            # [ä¿®å¤] è¡¥å……å®Œæ•´å­—æ®µï¼šf7=æŒ¯å¹…, f8=æ¢æ‰‹ç‡
            "fields": "f12,f14,f2,f3,f4,f5,f6,f7,f8,f15,f16,f17,f18",
            "_": str(int(time.time() * 1000))
        }

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            # [ä¿®å¤] Referer å»é™¤ç©ºæ ¼
            "Referer": "http://quote.eastmoney.com/",
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "X-Requested-With": "XMLHttpRequest",
        }

        logger.info("ğŸš€ [é»‘ç§‘æŠ€] æ­£åœ¨ä¼ªè£… Chrome è¯·æ±‚ä¸œè´¢å…¨å¸‚åœºæ•°æ®...")
        
        try:
            # [å…³é”®] impersonate="chrome120" è®©æœåŠ¡å™¨è®¤ä¸ºè¿™æ˜¯çœŸå®æµè§ˆå™¨
            r = self.session.get(
                url, 
                params=params, 
                headers=headers, 
                timeout=15
            )
            
            if r.status_code != 200:
                logger.error(f"âŒ è¯·æ±‚è¿”å›çŠ¶æ€ç : {r.status_code}, å“åº”: {r.text[:200]}")
                return None

            data_json = r.json()
            
            # éªŒè¯æ•°æ®ç»“æ„
            if not data_json or data_json.get('rc') != 0:
                logger.error(f"âŒ ä¸œè´¢è¿”å›é”™è¯¯: {data_json.get('rt', 'æœªçŸ¥é”™è¯¯')}")
                return None
                
            if 'data' not in data_json or 'diff' not in data_json['data']:
                logger.error("âŒ ä¸œè´¢è¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸")
                return None
                
            raw_list = data_json['data']['diff']
            
            if not raw_list:
                logger.warning("âš ï¸ ä¸œè´¢è¿”å›ç©ºåˆ—è¡¨")
                return None
            
            # è½¬æ¢ä¸º DataFrame
            df = pd.DataFrame(raw_list)
            
            # [ä¿®å¤] å®Œæ•´å­—æ®µæ˜ å°„
            rename_map = {
                'f12': 'code',          # ä»£ç 
                'f14': 'name',          # åç§°
                'f2': 'close',          # æœ€æ–°ä»·
                'f3': 'pct_change',     # æ¶¨è·Œå¹…(%)
                'f4': 'change',         # æ¶¨è·Œé¢
                'f5': 'volume',         # æˆäº¤é‡(æ‰‹)
                'f6': 'amount',         # æˆäº¤é¢
                'f7': 'amplitude',      # [æ–°å¢] æŒ¯å¹…(%)
                'f8': 'turnover_rate',  # [ä¿®å¤] æ¢æ‰‹ç‡(%)
                'f17': 'open',          # å¼€ç›˜ä»·
                'f15': 'high',          # æœ€é«˜ä»·
                'f16': 'low',           # æœ€ä½ä»·
                'f18': 'pre_close',     # æ˜¨æ”¶
            }
            
            # åªé‡å‘½åå­˜åœ¨çš„åˆ—
            existing_cols = {k: v for k, v in rename_map.items() if k in df.columns}
            df.rename(columns=existing_cols, inplace=True)
            
            # ç¡®ä¿ code æ˜¯å­—ç¬¦ä¸²
            df['code'] = df['code'].astype(str).str.strip()
            
            logger.info(f"âœ… è·å–åˆ° {len(df)} æ¡æ•°æ®ï¼Œå­—æ®µ: {list(df.columns)}")
            return df.set_index('code')

        except RequestException as e:
            logger.error(f"âŒ [curl_cffi] ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            raise  # è®©é‡è¯•è£…é¥°å™¨å¤„ç†
        except Exception as e:
            logger.error(f"âŒ [curl_cffi] å¤„ç†å¤±è´¥: {e}")
            return None

    def _init_spot_data(self) -> bool:
        """åˆå§‹åŒ–å…¨å¸‚åœºæ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        today_str = get_beijing_time().strftime("%Y-%m-%d")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.spot_data_cache is not None and self.spot_data_date == today_str:
            logger.info("âœ… ä½¿ç”¨ä»Šæ—¥å·²ç¼“å­˜çš„ Spot æ•°æ®")
            return True

        # è·å–æ–°æ•°æ®
        df = self._fetch_eastmoney_raw_spot()
        if df is not None and not df.empty:
            self.spot_data_cache = df
            self.spot_data_date = today_str
            logger.info(f"âœ… å…¨å¸‚åœºå¿«ç…§ç¼“å­˜æˆåŠŸï¼Œå…± {len(df)} æ¡")
            return True
        
        logger.error("âŒ æ— æ³•è·å–å…¨å¸‚åœºæ•°æ®")
        return False

    def _safe_float(self, val, default: float = 0.0) -> float:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        if val is None or val == '-' or val == '':
            return default
        try:
            return float(val)
        except (ValueError, TypeError):
            return default

    def update_cache(self, fund_code: str) -> bool:
        """æ›´æ–°å•ä¸ªåŸºé‡‘ç¼“å­˜"""
        fetch_time = get_beijing_time().strftime("%Y-%m-%d %H:%M:%S")
        
        # 1. ç¡®ä¿å…¨é‡æ•°æ®å·²åŠ è½½
        if self.spot_data_cache is None:
            if not self._init_spot_data():
                return False

        # 2. æŸ¥æ‰¾æ•°æ®ï¼ˆå…¼å®¹å¸¦å¸‚åœºå‰ç¼€çš„ä»£ç ï¼‰
        clean_code = str(fund_code).strip()
        if len(clean_code) > 6:
            clean_code = clean_code[-6:]
            
        if clean_code not in self.spot_data_cache.index:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {fund_code} (clean: {clean_code})")
            return False

        try:
            row = self.spot_data_cache.loc[clean_code]
            
            # 3. æ„é€ å½“æ—¥ DataFrame
            today_date = pd.Timestamp(get_beijing_time().date())
            
            new_data = {
                'date': today_date,
                'open': self._safe_float(row.get('open')),
                'high': self._safe_float(row.get('high')),
                'low': self._safe_float(row.get('low')),
                'close': self._safe_float(row.get('close')),
                'volume': self._safe_float(row.get('volume')),
                'amount': self._safe_float(row.get('amount')),
                'amplitude': self._safe_float(row.get('amplitude')),  # [æ–°å¢]
                'pct_change': self._safe_float(row.get('pct_change')),
                'change': self._safe_float(row.get('change')),
                'turnover_rate': self._safe_float(row.get('turnover_rate')),
                'fetch_time': fetch_time,
                'source': 'eastmoney_spot'
            }
            
            df_new = pd.DataFrame([new_data])
            df_new.set_index('date', inplace=True)

            # 4. æ‹¼æ¥åˆ°æœ¬åœ° CSV
            file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            
            if os.path.exists(file_path):
                try:
                    df_old = pd.read_csv(file_path, index_col='date', parse_dates=['date'])
                    
                    # æ›´æ–°æˆ–è¿½åŠ 
                    if today_date in df_old.index:
                        df_old.update(df_new)
                        df_final = df_old
                    else:
                        df_final = pd.concat([df_old, df_new])
                    
                    # å»é‡æ’åº
                    df_final = df_final[~df_final.index.duplicated(keep='last')]
                    df_final.sort_index(inplace=True)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–å†å²æ•°æ®å¤±è´¥ï¼Œä½¿ç”¨æ–°æ•°æ®: {e}")
                    df_final = df_new
            else:
                df_final = df_new

            # æ ‡å‡†åŒ–å¹¶ä¿å­˜
            final_df = self._standardize_dataframe(df_final, "ä¸œè´¢")
            final_df.to_csv(file_path)
            
            logger.info(f"ğŸ’¾ [ä¸œè´¢] {fund_code} æ›´æ–°æˆåŠŸ (æ”¶ç›˜ä»·: {new_data['close']:.3f}, æ¶¨è·Œ: {new_data['pct_change']:.2f}%)")
            return True

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ•°æ®å¼‚å¸¸ {fund_code}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    print("ğŸš€ [DataFetcher] å¯åŠ¨ (curl_cffi ä¸œè´¢ä¸“ç”¨ç‰ˆ V18.1)...")
    
    # åŠ è½½é…ç½®
    try:
        with open('config.yaml', 'r', encoding='utf-8') as f:
            cfg = yaml.safe_load(f)
            funds = cfg.get('funds', [])
    except Exception as e:
        logger.error(f"è¯»å–é…ç½®å¤±è´¥: {e}")
        funds = []
    
    if not funds:
        print("âš ï¸ æœªæ‰¾åˆ° config.yaml æˆ–åŸºé‡‘åˆ—è¡¨ä¸ºç©º")
        exit(1)

    fetcher = DataFetcher()
    
    # åˆå§‹åŒ–å…¨å¸‚åœºæ•°æ®ï¼ˆåªè¯·æ±‚1æ¬¡ï¼‰
    if not fetcher._init_spot_data():
        logger.error("âŒ æ— æ³•è·å–å…¨å¸‚åœºæ•°æ®ï¼Œé€€å‡º")
        exit(1)

    # æ‰¹é‡æ›´æ–°
    success_count = 0
    total = len(funds)
    
    for idx, fund in enumerate(funds):
        code = str(fund.get('code', '')).strip()
        name = fund.get('name', 'Unknown')
        
        if not code or len(code) < 6:
            logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆä»£ç : {fund}")
            continue
            
        logger.info(f"ğŸ”„ [{idx+1}/{total}] {name} ({code})")
        
        if fetcher.update_cache(code):
            success_count += 1
            
        # æ¯ 10 ä¸ªè¾“å‡ºè¿›åº¦
        if (idx + 1) % 10 == 0 or idx == total - 1:
            logger.info(f"ğŸ“Š è¿›åº¦: {idx+1}/{total}, æˆåŠŸ: {success_count}")
            
    logger.info(f"ğŸ å®Œæˆ: {success_count}/{total}")
    print(f"ğŸ è¡Œæƒ…æ›´æ–°å®Œæˆ: {success_count}/{total}")
    
    # æ¸…ç†
    del fetcher
    gc.collect()
