import pandas as pd
import time
import random
import os
import yaml
import logging
import gc
import sys
from datetime import datetime, time as dt_time
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass

# æ£€æŸ¥ä¾èµ–
try:
    from curl_cffi import requests as cffi_requests
    from curl_cffi.requests.exceptions import RequestException, ProxyError, Timeout
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install curl_cffi pandas pyyaml")
    sys.exit(1)

# ===================== é…ç½® =====================
def get_beijing_time():
    from datetime import timezone, timedelta
    return datetime.now(timezone(timedelta(hours=8)))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class Proxy:
    """ä»£ç†é…ç½®"""
    host: str
    port: int
    user: str
    password: str
    location: str = ""
    
    def __str__(self):
        # æ³¨æ„ï¼šcurl_cffi æ¥å—çš„ä»£ç†æ ¼å¼ä¸º scheme://user:pass@host:port
        return f"http://{self.user}:{self.password}@{self.host}:{self.port}"
    
    def to_dict(self):
        url = str(self)
        return {"http": url, "https": url}

# [ä»£ç†æ± ] å¤šä¸ªä»£ç†é…ç½®ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
# âš ï¸ æ³¨æ„ï¼šè¯·ç¡®ä¿è¿™äº›ä»£ç† IP å’Œè´¦å·å¯†ç å½“å‰æ˜¯æœ‰æ•ˆçš„ï¼Œå¦åˆ™ä¼šå¯¼è‡´å…¨éƒ¨è¯·æ±‚å¤±è´¥
PROXY_POOL = [
    Proxy("31.59.20.176", 6754, "typembrv", "kx2q7wpv1dd4", "ğŸ‡¬ğŸ‡§ä¼¦æ•¦"),
    Proxy("23.95.150.145", 6114, "typembrv", "kx2q7wpv1dd4", "ğŸ‡¬ğŸ‡§ä¼¦æ•¦"),
    Proxy("198.23.239.134", 6540, "typembrv", "kx2q7wpv1dd4", "ğŸ‡ºğŸ‡¸æ°´ç‰›"),
    Proxy("45.38.107.97", 6014, "typembrv", "kx2q7wpv1dd4", "ğŸ‡ºğŸ‡¸æ°´ç‰›"),
    Proxy("107.172.163.27", 6543, "typembrv", "kx2q7wpv1dd4", "ğŸ‡¬ğŸ‡§ä¼¦æ•¦"),
    Proxy("198.105.121.200", 6462, "typembrv", "kx2q7wpv1dd4", "ğŸ‡ºğŸ‡¸å¸ƒå¢æ˜æˆ´å°”"),
    Proxy("64.137.96.74", 6641, "typembrv", "kx2q7wpv1dd4", "ğŸ‡¬ğŸ‡§ä¼¦æ•¦é‡‘èåŸ"),
    Proxy("216.10.27.159", 6837, "typembrv", "kx2q7wpv1dd4", "ğŸ‡ªğŸ‡¸é©¬å¾·é‡Œ"),
    Proxy("23.26.71.145", 5628, "typembrv", "kx2q7wpv1dd4", "ğŸ‡ºğŸ‡¸è¾¾æ‹‰æ–¯"),
    Proxy("23.229.19.94", 8689, "typembrv", "kx2q7wpv1dd4", "ğŸ‡ºğŸ‡¸å¥¥å‹’å§†"),
]

class ProxyManager:
    """ä»£ç†ç®¡ç†å™¨ï¼šè‡ªåŠ¨è½®è¯¢å’Œæ•…éšœè½¬ç§»"""
    
    def __init__(self, proxies: List[Proxy]):
        self.proxies = proxies
        self.current_index = 0
        self.failed_proxies: set = set()  # è®°å½•å¤±è´¥çš„ä»£ç†
        self.last_used: Dict[str, float] = {}  # è®°å½•ä¸Šæ¬¡ä½¿ç”¨æ—¶é—´
        
    def get_next_proxy(self) -> Optional[Proxy]:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨ä»£ç†"""
        attempts = 0
        while attempts < len(self.proxies):
            proxy = self.proxies[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.proxies)
            
            # è·³è¿‡å·²å¤±è´¥çš„ä»£ç†
            if proxy.host in self.failed_proxies:
                attempts += 1
                continue
            
            # æ£€æŸ¥å†·å´æ—¶é—´ï¼ˆåŒä¸€ä»£ç†è‡³å°‘é—´éš” 1 ç§’ï¼Œé¿å…è¿‡äºé¢‘ç¹ï¼‰
            last_time = self.last_used.get(proxy.host, 0)
            if time.time() - last_time < 1:
                time.sleep(1)
            
            self.last_used[proxy.host] = time.time()
            return proxy
            
            attempts += 1
        
        # æ‰€æœ‰ä»£ç†éƒ½å¤±è´¥äº†ï¼Œé‡ç½®å¹¶å†è¯•ä¸€æ¬¡
        if self.failed_proxies:
            logger.warning("ğŸ”„ æ‰€æœ‰ä»£ç†éƒ½å¤±è´¥è¿‡ï¼Œé‡ç½®å¤±è´¥åˆ—è¡¨é‡è¯•...")
            self.failed_proxies.clear()
            # é€’å½’è°ƒç”¨
            return self.get_next_proxy()
        
        return None
    
    def mark_failed(self, proxy: Proxy):
        """æ ‡è®°ä»£ç†ä¸ºå¤±è´¥"""
        logger.warning(f"âŒ ä»£ç† {proxy.location} {proxy.host} æ ‡è®°ä¸ºå¤±è´¥")
        self.failed_proxies.add(proxy.host)
    
    def get_proxy_count(self):
        return len(self.proxies)

# ===================== DataFetcher =====================
class DataFetcher:
    UNIFIED_COLUMNS = ['date', 'open', 'high', 'low', 'close', 'volume',
                       'amount', 'amplitude', 'pct_change', 'change', 'turnover_rate', 'fetch_time']
    
    def __init__(self):
        self.DATA_DIR = "data_cache"
        os.makedirs(self.DATA_DIR, exist_ok=True)
        
        self.spot_data_cache: Optional[pd.DataFrame] = None
        self.spot_data_date: Optional[str] = None
        self.session: Optional[cffi_requests.Session] = None
        self.proxy_manager = ProxyManager(PROXY_POOL)
        self.current_proxy: Optional[Proxy] = None
        
        # ç»Ÿè®¡
        self.total_funds = 0
        self.success_count = 0

    def _create_session(self, proxy: Proxy):
        """ä½¿ç”¨æŒ‡å®šä»£ç†åˆ›å»º session"""
        self._close_session() # ç¡®ä¿æ—§çš„è¢«æ¸…ç†
        
        try:
            self.current_proxy = proxy
            self.session = cffi_requests.Session(
                impersonate="chrome120",
                proxies=proxy.to_dict(),
                timeout=30
            )
            logger.info(f"ğŸŒ åˆ‡æ¢ä»£ç†: {proxy.location} {proxy.host}:{proxy.port}")
            time.sleep(random.uniform(0.5, 1))
        except Exception as e:
            logger.error(f"âŒ åˆ›å»º session å¤±è´¥: {e}")
            raise

    def _close_session(self):
        if self.session:
            try:
                self.session.close()
            except:
                pass
            self.session = None
            gc.collect()

    def _safe_request(self, url: str, params: dict, headers: dict, max_proxy_retries: int = 3) -> Optional[dict]:
        """
        å¸¦ä»£ç†åˆ‡æ¢çš„é‡è¯•æœºåˆ¶ - ä¼˜åŒ–ç‰ˆ
        ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨å½“å‰ sessionï¼Œå¤±è´¥æ‰åˆ‡æ¢ä»£ç†
        """
        # å¦‚æœå½“å‰æ²¡æœ‰ sessionï¼Œå…ˆåˆå§‹åŒ–ä¸€ä¸ª
        if self.session is None:
            proxy = self.proxy_manager.get_next_proxy()
            if not proxy:
                logger.error("âŒ å¯åŠ¨å¤±è´¥ï¼šæ²¡æœ‰å¯ç”¨ä»£ç†")
                return None
            self._create_session(proxy)

        # å°è¯•å¾ªç¯
        for attempt in range(max_proxy_retries):
            try:
                # ä½¿ç”¨å½“å‰ session å‘èµ·è¯·æ±‚
                if not self.session:
                    raise Exception("Sessionä¸¢å¤±")
                    
                r = self.session.get(url, params=params, headers=headers)
                r.raise_for_status()
                return r.json()
                
            except (ProxyError, Timeout, RequestException, Exception) as e:
                # è®°å½•é”™è¯¯
                err_msg = str(e)[:100]
                proxy_info = f"{self.current_proxy.host}" if self.current_proxy else "Unknown"
                logger.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ (ä»£ç†: {proxy_info}): {err_msg}")
                
                # æ ‡è®°å½“å‰ä»£ç†æœ‰é—®é¢˜ï¼ˆå¦‚æœæ˜¯ä»£ç†é”™è¯¯ï¼‰
                if self.current_proxy and (isinstance(e, ProxyError) or isinstance(e, Timeout)):
                    self.proxy_manager.mark_failed(self.current_proxy)
                
                # è·å–æ–°ä»£ç†å¹¶é‡å»º Session
                new_proxy = self.proxy_manager.get_next_proxy()
                if not new_proxy:
                    logger.error("âŒ ä»£ç†æ± è€—å°½")
                    return None
                    
                self._create_session(new_proxy)
                # ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯å°è¯•
        
        logger.error(f"âŒ å·²é‡è¯• {max_proxy_retries} æ¬¡ï¼Œå…¨éƒ¨å¤±è´¥")
        return None

    def fetch_all_etfs(self) -> Optional[pd.DataFrame]:
        """è·å–å…¨å¸‚åœº ETF æ•°æ®"""
        url = "https://push2.eastmoney.com/api/qt/clist/get"
        
        all_data = []
        page = 1
        consecutive_errors = 0  # è¿ç»­é”™è¯¯è®¡æ•°
        
        logger.info("ğŸ“¡ å¼€å§‹è·å– ETF å…¨é‡åˆ—è¡¨...")
        
        while page <= 200 and consecutive_errors < 3:
            if page % 10 == 0:
                logger.info(f"ğŸ“„ è·å–ç¬¬ {page} é¡µ...")
            
            params = {
                "pn": str(page),
                "pz": "100",
                "po": "1",
                "np": "1",
                "ut": "bd1d9ddb04089700cf9c27f6f7426281",
                "fltt": "2",
                "invt": "2",
                "fid": "f3",
                "fs": "b:MK0021,b:MK0022,b:MK0023,b:MK0024",
                "fields": "f12,f14,f2,f3,f4,f5,f6,f7,f8,f15,f16,f17,f18",
                "_": str(int(time.time() * 1000))
            }
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Referer": "http://quote.eastmoney.com/",
                "Accept": "application/json, text/javascript, */*; q=0.01",
            }
            
            data = self._safe_request(url, params, headers, max_proxy_retries=5)
            
            if not data or data.get('rc') != 0 or 'data' not in data or 'diff' not in data['data']:
                consecutive_errors += 1
                logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæ•°æ®å¼‚å¸¸ (è¿ç»­é”™è¯¯ {consecutive_errors}/3)")
                if consecutive_errors >= 3:
                    logger.error("âŒ è¿ç»­ 3 é¡µé”™è¯¯ï¼Œç»ˆæ­¢è·å–")
                    break
                continue
            
            # æˆåŠŸï¼Œé‡ç½®é”™è¯¯è®¡æ•°
            consecutive_errors = 0
            items = data['data']['diff']
            
            if not items:
                break
                
            all_data.extend(items)
            
            if len(items) < 100:
                break
            
            page += 1
            # åªæœ‰åœ¨éå¸¸å¿«çš„æ—¶å€™æ‰ç¨å¾®æš‚åœï¼Œä»£ç†æ¨¡å¼ä¸‹ä¸éœ€è¦å¤ªä¹…çš„ sleep
            if page % 5 == 0:
                time.sleep(random.uniform(0.5, 1.5)) 
        
        self._close_session()
        
        if not all_data:
            return None
        
        # å¤„ç†æ•°æ®
        df = pd.DataFrame(all_data)
        
        rename_map = {
            'f12': 'code', 'f14': 'name', 'f2': 'close', 'f3': 'pct_change',
            'f4': 'change', 'f5': 'volume', 'f6': 'amount', 'f7': 'amplitude',
            'f8': 'turnover_rate', 'f17': 'open', 'f15': 'high', 'f16': 'low',
        }
        
        df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns}, inplace=True)
        
        # æ¸…ç†ä»£ç æ ¼å¼
        df['code'] = df['code'].astype(str).str.strip().str.lower().str.replace(r'^(sh|sz)', '', regex=True)
        df = df.drop_duplicates(subset=['code'], keep='first')
        
        logger.info(f"âœ… å…±è·å– {len(df)} åª ETF")
        return df.set_index('code')

    def init_spot_data(self) -> bool:
        """åˆå§‹åŒ–æ•°æ®ç¼“å­˜"""
        today = get_beijing_time().strftime("%Y-%m-%d")
        
        if self.spot_data_cache is not None and self.spot_data_date == today:
            logger.info("âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return True
        
        df = self.fetch_all_etfs()
        if df is not None and not df.empty:
            self.spot_data_cache = df
            self.spot_data_date = today
            return True
        
        return False

    def update_single(self, fund_code: str) -> bool:
        """æ›´æ–°å•ä¸ªåŸºé‡‘"""
        if self.spot_data_cache is None:
            if not self.init_spot_data():
                return False
        
        code = str(fund_code).strip().lower().replace('sh', '').replace('sz', '')
        
        if code not in self.spot_data_cache.index:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {fund_code}ï¼Œå¯èƒ½æ˜¯åœç‰Œæˆ–é ETF")
            return False
        
        try:
            row = self.spot_data_cache.loc[code]
            today = pd.Timestamp(get_beijing_time().date())
            
            def to_float(x):
                try:
                    return float(x) if x and x != '-' else 0.0
                except:
                    return 0.0
            
            new_data = {
                'date': today,
                'open': to_float(row.get('open')),
                'high': to_float(row.get('high')),
                'low': to_float(row.get('low')),
                'close': to_float(row.get('close')),
                'volume': to_float(row.get('volume')),
                'amount': to_float(row.get('amount')),
                'amplitude': to_float(row.get('amplitude')),
                'pct_change': to_float(row.get('pct_change')),
                'change': to_float(row.get('change')),
                'turnover_rate': to_float(row.get('turnover_rate')),
                'fetch_time': get_beijing_time().strftime("%Y-%m-%d %H:%M:%S"),
                'source': 'eastmoney_spot'
            }
            
            df_new = pd.DataFrame([new_data])
            df_new.set_index('date', inplace=True)
            
            # åˆå¹¶å†å²æ•°æ®
            path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            if os.path.exists(path):
                try:
                    df_old = pd.read_csv(path, index_col='date', parse_dates=['date'])
                    # å¦‚æœä»Šå¤©çš„æ•°æ®å·²å­˜åœ¨ï¼Œä½¿ç”¨ update æ›´æ–°ï¼›å¦åˆ™ concat è¿½åŠ 
                    if today in df_old.index:
                        df_old.update(df_new)
                        df_final = df_old
                    else:
                        df_final = pd.concat([df_old, df_new])
                    
                    df_final = df_final[~df_final.index.duplicated(keep='last')].sort_index()
                except Exception as e:
                    logger.error(f"è¯»å–æ—§æ–‡ä»¶å‡ºé”™ {path}: {e}")
                    df_final = df_new
            else:
                df_final = df_new
            
            # æ ‡å‡†åŒ–å¹¶ä¿å­˜
            df_final = df_final.reindex(columns=self.UNIFIED_COLUMNS)
            for col in ['open', 'high', 'low', 'close', 'volume', 'amount', 
                        'amplitude', 'pct_change', 'change', 'turnover_rate']:
                df_final[col] = pd.to_numeric(df_final[col], errors='coerce')
            
            df_final.to_csv(path)
            logger.info(f"ğŸ’¾ {fund_code} æˆåŠŸ (æ”¶ç›˜: {new_data['close']:.3f})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {fund_code} å¤„ç†å¤±è´¥: {e}")
            return False

    def run(self, funds: List[dict]):
        """æ‰¹é‡è¿è¡Œ"""
        self.total_funds = len(funds)
        self.success_count = 0
        
        # é¢„å…ˆæµ‹è¯•ä»£ç†è¿æ¥
        logger.info("ğŸ” æ­£åœ¨æµ‹è¯•ä»£ç†è¿é€šæ€§...")
        test_res = self._safe_request("https://www.baidu.com", {}, {}, max_proxy_retries=3)
        if not test_res and not self.session:
            logger.error("âŒ æ— æ³•è¿æ¥ç½‘ç»œï¼Œè¯·æ£€æŸ¥ä»£ç†é…ç½®")
            # å³ä½¿ç™¾åº¦æµ‹è¯•å¤±è´¥ä¹Ÿå°è¯•ç»§ç»­ï¼Œå¯èƒ½æ˜¯ç™¾åº¦è¢«å¢™ï¼Œä½†ä¸œè´¢èƒ½é€š
        
        if not self.init_spot_data():
            logger.error("âŒ åˆå§‹åŒ–æ•°æ®è·å–å¤±è´¥ï¼Œé€€å‡º")
            return 0
        
        for i, fund in enumerate(funds, 1):
            code = str(fund.get('code', '')).strip()
            name = fund.get('name', 'Unknown')
            
            if not code or len(code) < 6:
                continue
            
            # logger.info(f"ğŸ”„ [{i}/{self.total_funds}] {name} ({code})")
            
            if self.update_single(code):
                self.success_count += 1
            
            if i % 50 == 0:
                 logger.info(f"ğŸ“Š è¿›åº¦: {i}/{self.total_funds}, æˆåŠŸ: {self.success_count}")
        
        return self.success_count

# ===================== ä¸»å…¥å£ =====================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ DataFetcher V22.1 (Optimized) - ä¸œè´¢ ETF æ•°æ®è·å–")
    print(f"ğŸŒ ä»£ç†æ± : {len(PROXY_POOL)} ä¸ªèŠ‚ç‚¹")
    for i, p in enumerate(PROXY_POOL[:3], 1):
        print(f"   {i}. {p.location} {p.host}:{p.port}")
    if len(PROXY_POOL) > 3:
        print(f"   ... ç­‰å…± {len(PROXY_POOL)} ä¸ª")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    try:
        # å¦‚æœæ²¡æœ‰é…ç½®æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªå‡çš„ç”¨äºæµ‹è¯•
        if not os.path.exists('config.yaml'):
            logger.warning("âš ï¸ æœªæ‰¾åˆ° config.yamlï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®")
            funds = [{'code': '510300', 'name': 'æ²ªæ·±300ETF'}, {'code': '510050', 'name': 'ä¸Šè¯50ETF'}]
        else:
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                funds = cfg.get('funds', [])
    except Exception as e:
        logger.error(f"è¯»å– config.yaml å¤±è´¥: {e}")
        funds = []
    
    if not funds:
        print("âŒ æœªæ‰¾åˆ°åŸºé‡‘åˆ—è¡¨")
        sys.exit(1)
    
    # è¿è¡Œ
    fetcher = DataFetcher()
    success = fetcher.run(funds)
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ å®Œæˆ: {success}/{len(funds)} ({success/len(funds)*100:.1f}%)")
    print(f"{'=' * 60}")
    
    sys.exit(0 if success > 0 else 1)
