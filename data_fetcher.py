import akshare as ak
import pandas as pd
import time
import random
import os
import yaml
from datetime import datetime, time as dt_time
import logging

# ===================== ä¸´æ—¶è¡¥å…… utils æ¨¡å—ç¼ºå¤±çš„éƒ¨åˆ†ï¼ˆå¦‚æœéœ€è¦ï¼‰ =====================
def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰"""
    from datetime import timezone, timedelta
    return datetime.now(timezone(timedelta(hours=8)))

# ç®€æ˜“æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def retry(retries=3, delay=5):
    """ç®€æ˜“é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if i == retries - 1:
                        raise e
                    time.sleep(delay)
            return None
        return wrapper
    return decorator
# ====================================================================================

class DataFetcher:
    # [V15.17] ç»Ÿä¸€å­—æ®µè§„èŒƒï¼ˆæ‰€æœ‰æ•°æ®æºè¿”å›çš„å­—æ®µç»“æ„ï¼‰
    UNIFIED_COLUMNS = [
        'date', 'open', 'high', 'low', 'close', 'volume',
        'amount', 'amplitude', 'pct_change', 'change', 'turnover_rate',
        'fetch_time'  # æ•°æ®æŠ“å–æ—¶é—´
    ]
    
    def __init__(self):
        self.DATA_DIR = "data_cache"
        if not os.path.exists(self.DATA_DIR):
            os.makedirs(self.DATA_DIR)
            
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]

    def _verify_data_freshness(self, df, fund_code, source_name):
        """æ•°æ®æ–°é²œåº¦å®¡è®¡ (é€šç”¨)"""
        if df is None or df.empty: 
            return
        
        try:
            last_date = pd.to_datetime(df.index[-1]).date()
            now_bj = get_beijing_time()
            today_date = now_bj.date()
            is_trading_time = (dt_time(9, 30) <= now_bj.time() <= dt_time(15, 0))
            
            log_prefix = f"ğŸ“… [{source_name}] {fund_code} æœ€æ–°æ—¥æœŸ: {last_date}"
            
            if last_date == today_date:
                logger.info(f"{log_prefix} | âœ… æ•°æ®å·²æ›´æ–°è‡³ä»Šæ—¥")
            elif last_date < today_date:
                days_gap = (today_date - last_date).days
                if is_trading_time and days_gap >= 1:
                    logger.warning(f"{log_prefix} | âš ï¸ æ•°æ®æ»å {days_gap} å¤© (è¯·è¿è¡Œçˆ¬è™«æ›´æ–°)")
                else:
                    logger.info(f"{log_prefix} | â¸ï¸ å†å²æ•°æ®å°±ç»ª")
        except Exception as e:
            logger.warning(f"å®¡è®¡æ•°æ®æ–°é²œåº¦å¤±è´¥: {e}")

    def _standardize_dataframe(self, df, source_name):
        """
        [V15.17] æ ‡å‡†åŒ– DataFrameï¼šç¡®ä¿æ‰€æœ‰æ•°æ®æºè¿”å›ç»Ÿä¸€çš„å­—æ®µç»“æ„
        ç¼ºå¤±å­—æ®µå¡«å……ä¸º NaN
        """
        if df is None or df.empty:
            return df
            
        # ç¡®ä¿æ‰€æœ‰ç»Ÿä¸€å­—æ®µéƒ½å­˜åœ¨ï¼Œç¼ºå¤±çš„å¡«å……ä¸º NaN
        for col in self.UNIFIED_COLUMNS:
            if col not in df.columns:
                df[col] = pd.NA
        
        # æŒ‰ç»Ÿä¸€é¡ºåºæ’åˆ—åˆ—
        df = df[self.UNIFIED_COLUMNS]
        
        # æ•°æ®ç±»å‹è½¬æ¢
        numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 
                       'amplitude', 'pct_change', 'change', 'turnover_rate']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        return df

    @retry(retries=3, delay=5)
    def _fetch_from_network(self, fund_code):
        """
        [ç§æœ‰æ–¹æ³•] çº¯è”ç½‘è·å–æ•°æ® (ä¸œè´¢ -> æ–°æµª -> è…¾è®¯)
        æ‰€æœ‰æ•°æ®æºç»Ÿä¸€è¿”å›æ ‡å‡†å­—æ®µç»“æ„
        """
        fetch_time = get_beijing_time().strftime("%Y-%m-%d %H:%M:%S")
        
        # 1. ä¸œè´¢ (EastMoney) - ä¼˜å…ˆæ•°æ®æºï¼Œå­—æ®µæœ€å…¨
        try:
            time.sleep(random.uniform(1.0, 2.0)) 
            df = ak.fund_etf_hist_em(
                symbol=fund_code, 
                period="daily", 
                start_date="20200101", 
                end_date="20500101", 
                adjust="qfq"
            )
            
            # ä¸œè´¢å­—æ®µæ˜ å°„ï¼ˆæœ€å…¨ï¼‰
            rename_map = {
                'æ—¥æœŸ': 'date',
                'å¼€ç›˜': 'open',
                'æ”¶ç›˜': 'close',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low',
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount',
                'æŒ¯å¹…': 'amplitude',
                'æ¶¨è·Œå¹…': 'pct_change',
                'æ¶¨è·Œé¢': 'change',
                'æ¢æ‰‹ç‡': 'turnover_rate'
            }
            df.rename(columns=rename_map, inplace=True)
            df['date'] = pd.to_datetime(df['date'])
            df.set_index('date', inplace=True)
            df['fetch_time'] = fetch_time
            df['source'] = 'eastmoney'
            
            df = self._standardize_dataframe(df, "ä¸œè´¢")
            if not df.empty: 
                return df, "ä¸œè´¢"
        except Exception as e:
            logger.error(f"ä¸œè´¢æ•°æ®æºå¼‚å¸¸: {e}")
            pass

        # 2. æ–°æµª (Sina) - å­—æ®µæœ‰é™ï¼Œç¼ºå¤±å­—æ®µå¡«å…… NaN
        try:
            time.sleep(1)
            df = ak.fund_etf_hist_sina(symbol=fund_code)
            
            if df.index.name in ['date', 'æ—¥æœŸ']: 
                df = df.reset_index()
            
            # æ–°æµªè¿”å›å­—æ®µï¼šæ—¥æœŸã€å¼€ç›˜ã€æ”¶ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æˆäº¤é‡ï¼ˆå­—æ®µåå¯èƒ½ä¸ºè‹±æ–‡æˆ–ä¸­æ–‡ï¼‰
            # éœ€è¦æ™ºèƒ½è¯†åˆ«åˆ—å
            col_mapping = {}
            for col in df.columns:
                col_str = str(col).lower()
                if col_str in ['date', 'æ—¥æœŸ']:
                    col_mapping[col] = 'date'
                elif col_str in ['open', 'å¼€ç›˜']:
                    col_mapping[col] = 'open'
                elif col_str in ['close', 'æ”¶ç›˜', 'latest']:
                    col_mapping[col] = 'close'
                elif col_str in ['high', 'æœ€é«˜']:
                    col_mapping[col] = 'high'
                elif col_str in ['low', 'æœ€ä½']:
                    col_mapping[col] = 'low'
                elif col_str in ['volume', 'æˆäº¤é‡', 'vol']:
                    col_mapping[col] = 'volume'
            
            df.rename(columns=col_mapping, inplace=True)
            
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                
                # æ–°æµªç¼ºå¤±å­—æ®µå¡«å……ä¸º NaN
                df['amount'] = pd.NA
                df['amplitude'] = pd.NA
                df['pct_change'] = pd.NA
                df['change'] = pd.NA
                df['turnover_rate'] = pd.NA
                df['fetch_time'] = fetch_time
                df['source'] = 'sina'
                
                # åŸºç¡€ç±»å‹æ¸…æ´—
                for col in ['open', 'high', 'low', 'close', 'volume']:
                    if col in df.columns: 
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                
                df = self._standardize_dataframe(df, "æ–°æµª")
                return df, "æ–°æµª"
        except Exception as e:
            logger.error(f"æ–°æµªæ•°æ®æºå¼‚å¸¸: {e}")
            pass

        # 3. è…¾è®¯ (Tencent) - å­—æ®µè¾ƒå…¨ï¼Œä¸ä¸œè´¢ç±»ä¼¼
        try:
            time.sleep(1)
            prefix = 'sh' if fund_code.startswith('5') else ('sz' if fund_code.startswith('1') else '')
            if prefix:
                df = ak.stock_zh_a_hist_tx(
                    symbol=f"{prefix}{fund_code}", 
                    start_date="20200101", 
                    adjust="qfq"
                )
                
                # è…¾è®¯å­—æ®µæ˜ å°„ï¼ˆä¸ä¸œè´¢ç±»ä¼¼ï¼‰
                rename_map = {
                    'æ—¥æœŸ': 'date',
                    'å¼€ç›˜': 'open',
                    'æ”¶ç›˜': 'close',
                    'æœ€é«˜': 'high',
                    'æœ€ä½': 'low',
                    'æˆäº¤é‡': 'volume',
                    'æˆäº¤é¢': 'amount',
                    'æŒ¯å¹…': 'amplitude',
                    'æ¶¨è·Œå¹…': 'pct_change',
                    'æ¶¨è·Œé¢': 'change',
                    'æ¢æ‰‹ç‡': 'turnover_rate'
                }
                df.rename(columns=rename_map, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                df['fetch_time'] = fetch_time
                df['source'] = 'tencent'
                
                df = self._standardize_dataframe(df, "è…¾è®¯")
                if not df.empty: 
                    return df, "è…¾è®¯"
        except Exception as e:
            logger.error(f"è…¾è®¯æ•°æ®æºå¼‚å¸¸: {e}")
            pass
        
        return None, None

    def update_cache(self, fund_code):
        """
        [çˆ¬è™«ä¸“ç”¨] è”ç½‘ä¸‹è½½æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ° CSV
        """
        df, source = self._fetch_from_network(fund_code)
        if df is not None and not df.empty:
            file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            df.to_csv(file_path)
            logger.info(f"ğŸ’¾ [{source}] {fund_code} æ•°æ®å·²ä¿å­˜è‡³ {file_path} (ç»Ÿä¸€å­—æ®µç»“æ„)")
            
            # [ä¼˜åŒ–] å¦‚æœæ˜¯ä¸œè´¢æ•°æ®ï¼Œå¼ºåˆ¶ç­‰å¾… 40 ç§’ï¼Œé˜²æ­¢æ¥å£å°ç¦
            if source == "ä¸œè´¢":
                logger.info("â³ [ä¸œè´¢] è§¦å‘é¢‘ç‡ä¿æŠ¤æœºåˆ¶ï¼Œç­‰å¾… 40 ç§’...")
                time.sleep(40)
                
            return True
        else:
            logger.error(f"âŒ {fund_code} æ‰€æœ‰æ•°æ®æº(ä¸œè´¢/æ–°æµª/è…¾è®¯)å‡è·å–å¤±è´¥")
            return False

    def get_fund_history(self, fund_code, days=250):
        """
        [ä¸»ç¨‹åºä¸“ç”¨] åªè¯»æ¨¡å¼ï¼šç›´æ¥ä»æœ¬åœ° CSV è¯»å–æ•°æ®
        """
        file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
        
        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ æœ¬åœ°ç¼“å­˜ç¼ºå¤±: {fund_code}ï¼Œè¯·ç­‰å¾… GitHub Action çˆ¬è™«è¿è¡Œ")
            return None
            
        try:
            df = pd.read_csv(file_path, index_col='date', parse_dates=['date'])
            
            # è§£ææŠ“å–æ—¶é—´å­—æ®µ
            if 'fetch_time' in df.columns:
                df['fetch_time'] = pd.to_datetime(df['fetch_time'])
            
            self._verify_data_freshness(df, fund_code, "æœ¬åœ°ç¼“å­˜")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–æœ¬åœ°ç¼“å­˜å¤±è´¥ {fund_code}: {e}")
            return None

# ==========================================
# [æ–°å¢] ç‹¬ç«‹è¿è¡Œå…¥å£ (è®©æ­¤è„šæœ¬å˜èº«çˆ¬è™«)
# ==========================================
if __name__ == "__main__":
    print("ğŸš€ [DataFetcher] å¯åŠ¨å¤šæºè¡Œæƒ…æŠ“å– (V15.17 Unified Fields)...")
    
    def load_config_local():
        try:
            with open('config.yaml', 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except:
            return {}

    cfg = load_config_local()
    funds = cfg.get('funds', [])
    
    if not funds:
        print("âš ï¸ æœªæ‰¾åˆ°åŸºé‡‘åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ config.yaml")
        exit()

    fetcher = DataFetcher()
    success_count = 0
    
    for fund in funds:
        code = fund.get('code')
        name = fund.get('name')
        print(f"ğŸ”„ æ›´æ–°: {name} ({code})...")
        
        try:
            if fetcher.update_cache(code):
                success_count += 1
            time.sleep(random.uniform(1.0, 2.0))
        except Exception as e:
            print(f"âŒ æ›´æ–°å¼‚å¸¸ {name}: {e}")
            
    print(f"ğŸ è¡Œæƒ…æ›´æ–°å®Œæˆ: {success_count}/{len(funds)} (ç»Ÿä¸€å­—æ®µç»“æ„)")
