import pandas as pd
import time
import random
import os
import yaml
import logging
import gc
import sys
from datetime import datetime, time as dt_time
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass

# æ£€æŸ¥ä¾èµ–
try:
    from curl_cffi import requests as cffi_requests
    from curl_cffi.requests.exceptions import RequestException, ProxyError, Timeout
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install curl_cffi pandas pyyaml")
    sys.exit(1)

# ===================== é…ç½®åŠ è½½ =====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

SCRAPERAPI_KEY = os.environ.get("SCRAPERAPI_KEY", "")
if not SCRAPERAPI_KEY:
    try:
        import settings
        SCRAPERAPI_KEY = getattr(settings, 'SCRAPERAPI_KEY', "")
    except ImportError: pass

ALLOW_DIRECT_FALLBACK = True 

if not SCRAPERAPI_KEY:
    logger.warning("âš ï¸ æœªæ£€æµ‹åˆ° SCRAPERAPI_KEYï¼Œå°†ä»…ä½¿ç”¨ç›´è¿žæ¨¡å¼")
else:
    masked_key = f"{SCRAPERAPI_KEY[:4]}****{SCRAPERAPI_KEY[-4:]}" if len(SCRAPERAPI_KEY) > 8 else "****"
    logger.info(f"ðŸ”‘ å·²åŠ è½½ ScraperAPI Key: {masked_key}")

def get_beijing_time():
    from datetime import timezone, timedelta
    return datetime.now(timezone(timedelta(hours=8)))

# ===================== DataFetcher =====================
class DataFetcher:
    UNIFIED_COLUMNS = ['date', 'open', 'high', 'low', 'close', 'volume',
                       'amount', 'amplitude', 'pct_change', 'change', 'turnover_rate', 'fetch_time']
    
    def __init__(self):
        self.DATA_DIR = "data_cache"
        os.makedirs(self.DATA_DIR, exist_ok=True)
        self.spot_data_cache: Optional[pd.DataFrame] = None
        self.spot_data_date: Optional[str] = None
        self.session: Optional[cffi_requests.Session] = None
        self.total_funds = 0
        self.success_count = 0
        self.target_codes = [] # å­˜å‚¨ç›®æ ‡åŸºé‡‘ä»£ç 
        
    def _get_scraperapi_proxy(self) -> str:
        return f"http://scraperapi:{SCRAPERAPI_KEY}@proxy-server.scraperapi.com:8001"

    def _create_session(self, use_proxy: bool = True):
        self._close_session()
        try:
            if use_proxy and SCRAPERAPI_KEY:
                proxy_url = self._get_scraperapi_proxy()
                self.session = cffi_requests.Session(
                    impersonate="chrome120",
                    proxies={"http": proxy_url, "https": proxy_url},
                    timeout=15,
                    verify=False 
                )
            else:
                self.session = cffi_requests.Session(
                    impersonate="chrome120",
                    timeout=15 
                )
        except Exception as e:
            logger.error(f"âŒ åˆ›å»º session å¤±è´¥: {e}")
            raise

    def _close_session(self):
        if self.session:
            try: self.session.close()
            except: pass
            self.session = None
            gc.collect()

    def _safe_request(self, url: str, params: dict, headers: dict, max_retries: int = 2) -> Optional[dict]:
        use_proxy_first = True if SCRAPERAPI_KEY else False
        if self.session is None:
            self._create_session(use_proxy=use_proxy_first)

        for attempt in range(max_retries + 1):
            try:
                if not self.session: raise Exception("Session Lost")
                r = self.session.get(url, params=params, headers=headers)
                
                if r.status_code == 403:
                    logger.warning("âš ï¸ 403 Forbidden (ä»£ç†é¢åº¦è€—å°½)")
                    if ALLOW_DIRECT_FALLBACK:
                         logger.info("ðŸ”„ 403 -> ç«‹å³åˆ‡æ¢ç›´è¿ž")
                         self._create_session(use_proxy=False)
                         r = self.session.get(url, params=params, headers=headers)
                         r.raise_for_status()
                         return r.json()
                    return None
                        
                r.raise_for_status()
                return r.json()
            except (ProxyError, Timeout, RequestException, Exception) as e:
                if ALLOW_DIRECT_FALLBACK:
                     self._create_session(use_proxy=False)
                else:
                     time.sleep(1)
        
        logger.error("âŒ æ‰€æœ‰å°è¯•å‡å¤±è´¥")
        return None

    # ðŸŸ¢ [æ ¸å¿ƒä¿®æ”¹] ç²¾å‡†æŠ“å–ï¼šåªæŠ“å– config ä¸­çš„åŸºé‡‘ï¼Œä¸å†ç¿»é¡µæŠ“å–å…¨å¸‚åœº
    def fetch_specific_etfs(self, codes: List[str]) -> Optional[pd.DataFrame]:
        url = "https://push2.eastmoney.com/api/qt/ulist.np/get"
        
        if not codes:
            logger.warning("âš ï¸ ç›®æ ‡ä»£ç åˆ—è¡¨ä¸ºç©º")
            return None

        # æž„é€  secids (ä¸œè´¢ä»£ç æ ¼å¼è½¬æ¢)
        # æ²ªå¸‚(5/6å¼€å¤´) -> 1.xxxxx
        # æ·±å¸‚(1/0/3å¼€å¤´) -> 0.xxxxx
        secids_list = []
        for code in codes:
            c = str(code).strip()
            if c.startswith('5') or c.startswith('6'):
                secids_list.append(f"1.{c}")
            else:
                secids_list.append(f"0.{c}")
        
        secids_str = ",".join(secids_list)
        
        logger.info(f"ðŸ“¡ æ­£åœ¨ç²¾å‡†æŠ“å– {len(codes)} åªåŸºé‡‘æ•°æ®...")
        
        params = {
            "fltt": "2",
            "secids": secids_str,
            "fields": "f12,f14,f2,f3,f4,f5,f6,f7,f8,f15,f16,f17,f18",
            "_": str(int(time.time() * 1000))
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        }
        
        # åªè¯·æ±‚ä¸€æ¬¡ï¼Œæ— éœ€å¾ªçŽ¯ç¿»é¡µ
        data = self._safe_request(url, params, headers, max_retries=3)
        
        if not data or 'diff' not in data.get('data', {}):
            logger.error("âŒ èŽ·å–å¤±è´¥: æ•°æ®ä¸ºç©º")
            return None
        
        items = data['data']['diff']
        logger.info(f"âœ… æˆåŠŸèŽ·å– {len(items)} æ¡æ•°æ®")
        
        df = pd.DataFrame(items)
        rename_map = {
            'f12': 'code', 'f14': 'name', 'f2': 'close', 'f3': 'pct_change',
            'f4': 'change', 'f5': 'volume', 'f6': 'amount', 'f7': 'amplitude',
            'f8': 'turnover_rate', 'f17': 'open', 'f15': 'high', 'f16': 'low',
        }
        df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns}, inplace=True)
        
        if 'code' in df.columns:
            df['code'] = df['code'].astype(str).str.strip().str.lower().str.replace(r'^(sh|sz)', '', regex=True)
            df = df.drop_duplicates(subset=['code'], keep='first')
            return df.set_index('code')
        else:
            return None

    def init_spot_data(self) -> bool:
        today = get_beijing_time().strftime("%Y-%m-%d")
        if self.spot_data_cache is not None and self.spot_data_date == today: return True
        
        # ðŸŸ¢ å…³é”®ï¼šä¼ å…¥ç›®æ ‡ä»£ç è¿›è¡Œç²¾å‡†æŠ“å–
        if self.target_codes:
            df = self.fetch_specific_etfs(self.target_codes)
        else:
            # å¦‚æžœæ²¡æœ‰ç›®æ ‡ä»£ç ï¼ˆç†è®ºä¸Šä¸ä¼šï¼‰ï¼Œå›žé€€åˆ°æ—§é€»è¾‘ï¼ˆä¸æŽ¨èï¼‰
            logger.warning("âš ï¸ æ— ç›®æ ‡ä»£ç ï¼Œå°è¯•å…¨é‡æŠ“å–(ä¸ç¨³å®š)...")
            # è¿™é‡Œç•™ç©ºï¼Œè®© update_single å¤„ç†
            return False

        if df is not None and not df.empty:
            self.spot_data_cache = df
            self.spot_data_date = today
            return True
        return False

    def update_single(self, fund_code: str) -> bool:
        # å¦‚æžœç¼“å­˜ä¸ºç©ºï¼Œå°è¯•åˆå§‹åŒ–
        if self.spot_data_cache is None:
            # å¦‚æžœ target_codes ä¸ºç©ºï¼Œä¸´æ—¶æŠŠå½“å‰è¿™ä¸€ä¸ªåŠ è¿›åŽ»å°è¯•æŠ“å–
            if not self.target_codes:
                self.target_codes = [fund_code]
            if not self.init_spot_data(): return False
        
        code = str(fund_code).strip().lower().replace('sh', '').replace('sz', '')
        
        if code not in self.spot_data_cache.index:
            # å¦‚æžœç¼“å­˜é‡Œæ²¡æœ‰ï¼Œå¯èƒ½æ˜¯æ¼äº†ï¼Œå°è¯•å•ç‹¬æŠ“è¿™ä¸€ä¸ª
            logger.warning(f"âš ï¸ ç¼“å­˜æœªå‘½ä¸­ {fund_code}ï¼Œå°è¯•å•ç‹¬è¡¥å½•...")
            df_single = self.fetch_specific_etfs([code])
            if df_single is not None and not df_single.empty:
                 self.spot_data_cache = pd.concat([self.spot_data_cache, df_single])
            else:
                 logger.error(f"âŒ æ— æ³•æ‰¾åˆ° {fund_code}")
                 return False
        
        try:
            row = self.spot_data_cache.loc[code]
            today = pd.Timestamp(get_beijing_time().date())
            
            def to_float(x):
                try: return float(x) if x and x != '-' else 0.0
                except: return 0.0
            
            new_data = {
                'date': today,
                'open': to_float(row.get('open')),
                'high': to_float(row.get('high')),
                'low': to_float(row.get('low')),
                'close': to_float(row.get('close')),
                'volume': to_float(row.get('volume')),
                'amount': to_float(row.get('amount')),
                'amplitude': to_float(row.get('amplitude')),
                'pct_change': to_float(row.get('pct_change')),
                'change': to_float(row.get('change')),
                'turnover_rate': to_float(row.get('turnover_rate')),
                'fetch_time': get_beijing_time().strftime("%Y-%m-%d %H:%M:%S"),
                'source': 'eastmoney_spot'
            }
            
            df_new = pd.DataFrame([new_data])
            df_new.set_index('date', inplace=True)
            
            path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            if os.path.exists(path):
                try:
                    df_old = pd.read_csv(path, index_col='date', parse_dates=['date'])
                    if today in df_old.index:
                        df_old.update(df_new)
                        df_final = df_old
                    else:
                        df_final = pd.concat([df_old, df_new])
                    df_final = df_final[~df_final.index.duplicated(keep='last')].sort_index()
                except:
                    df_final = df_new
            else:
                df_final = df_new
            
            df_final = df_final.reindex(columns=self.UNIFIED_COLUMNS)
            df_final.to_csv(path)
            return True
            
        except Exception as e:
            logger.error(f"âŒ {fund_code} å¤„ç†å¤±è´¥: {e}")
            return False

    def get_fund_history(self, fund_code: str) -> pd.DataFrame:
        code = str(fund_code).strip().lower().replace('sh', '').replace('sz', '')
        path = os.path.join(self.DATA_DIR, f"{code}.csv")
        
        if not os.path.exists(path):
            # logger.warning(f"âš ï¸ æœ¬åœ°æ— æ•°æ®ï¼Œå°è¯•æŠ“å– {fund_code}...")
            # ä¸´æ—¶æ·»åŠ ç›®æ ‡å¹¶æŠ“å–
            self.target_codes.append(fund_code)
            if not self.update_single(fund_code):
                return pd.DataFrame()
        
        try:
            df = pd.read_csv(path)
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
            df.sort_index(inplace=True)
            
            numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            return df
        except Exception as e:
            logger.error(f"âŒ è¯»å–åŽ†å²æ•°æ®å¤±è´¥ {fund_code}: {e}")
            return pd.DataFrame()

    def get_market_net_flow(self) -> float:
        """èŽ·å–å…¨å¸‚åœºèµ„é‡‘æµ"""
        try:
            url = "https://push2.eastmoney.com/api/qt/ulist.np/get"
            params = {
                "fltt": "2", "secids": "1.000001,0.399001", "fields": "f62",
                "_": str(int(time.time() * 1000))
            }
            headers = {"User-Agent": "Mozilla/5.0"}
            data = self._safe_request(url, params, headers)
            if not data or 'diff' not in data.get('data', {}): return 0.0
            
            total_flow = 0.0
            for item in data['data']['diff']:
                total_flow += float(item.get('f62', 0))
            return round(total_flow / 100000000, 2)
        except Exception as e:
            logger.error(f"âŒ èŽ·å–å®è§‚èµ„é‡‘æµå¤±è´¥: {e}")
            return 0.0

    def run(self, funds: List[dict]):
        self.total_funds = len(funds)
        self.success_count = 0
        
        # 1. æå–æ‰€æœ‰éœ€è¦æŠ“å–çš„ä»£ç 
        self.target_codes = [str(f.get('code')).strip() for f in funds if f.get('code')]
        
        logger.info("ðŸ” æ­£åœ¨åˆå§‹åŒ–...")
        flow = self.get_market_net_flow()
        logger.info(f"ðŸ’° [Macro] å…¨å¸‚åœºä¸»åŠ›å‡€æµå…¥: {flow} äº¿")

        # 2. ä¸€æ¬¡æ€§ç²¾å‡†æŠ“å–
        if not self.init_spot_data(): 
            logger.error("âŒ åˆå§‹åŒ–æŠ“å–å¤±è´¥")
            return 0
        
        # 3. éåŽ†ä¿å­˜
        for i, fund in enumerate(funds, 1):
            code = str(fund.get('code', '')).strip()
            if not code: continue
            if self.update_single(code):
                self.success_count += 1
            if i % 50 == 0:
                 logger.info(f"ðŸ“Š è¿›åº¦: {i}/{self.total_funds}, æˆåŠŸ: {self.success_count}")
        return self.success_count

if __name__ == "__main__":
    print("=" * 60)
    print("ðŸš€ DataFetcher V24.0 (Targeted Fetch Mode)")
    print("=" * 60)
    
    funds = []
    if os.path.exists('config.yaml'):
        try:
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                funds = cfg.get('funds', [])
        except Exception as e:
            logger.error(f"Config load error: {e}")
    
    if not funds:
        logger.warning("âš ï¸ config.yaml æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®")
        funds = [{'code': '510300', 'name': 'æ²ªæ·±300ETF'}, {'code': '510050', 'name': 'ä¸Šè¯50ETF'}]
    
    print(f"ðŸ“‹ è®¡åˆ’ç²¾å‡†æŠ“å– {len(funds)} åªåŸºé‡‘...")
    
    fetcher = DataFetcher()
    success = fetcher.run(funds)
    
    print(f"\n{'=' * 60}")
    print(f"ðŸ å®Œæˆ: {success}/{len(funds)}")
    print(f"{'=' * 60}")
    
    sys.exit(0 if success > 0 else 1)
