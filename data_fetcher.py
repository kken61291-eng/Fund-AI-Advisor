import akshare as ak
import pandas as pd
import time
import random
import os
import yaml
import logging
import requests
import gc
from datetime import datetime, time as dt_time, date

# ===================== å·¥å…·å‡½æ•° (ä¿æŒä¸å˜) =====================
def get_beijing_time():
    from datetime import timezone, timedelta
    return datetime.now(timezone(timedelta(hours=8)))

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def retry(retries=3, delay=10):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"âš ï¸ [Retry {i+1}/{retries}] æ“ä½œå¤±è´¥: {e}, ç­‰å¾… {delay}s åé‡è¯•...")
                    if i == retries - 1:
                        logger.error(f"âŒ é‡è¯•è€—å°½ï¼Œæœ€ç»ˆå¤±è´¥: {e}")
                        return None, None 
                    time.sleep(delay)
            return None
        return wrapper
    return decorator

def force_close_connections():
    try:
        if hasattr(ak, '_session') and ak._session:
            try:
                ak._session.close()
                ak._session = None
            except:
                pass
        gc.collect()
        time.sleep(0.5)
    except Exception as e:
        logger.debug(f"å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")

# ===================== DataFetcher ç±» (æ ¸å¿ƒä¿®æ”¹) =====================

class DataFetcher:
    UNIFIED_COLUMNS = [
        'date', 'open', 'high', 'low', 'close', 'volume',
        'amount', 'amplitude', 'pct_change', 'change', 'turnover_rate',
        'fetch_time'
    ]
    
    def __init__(self):
        self.DATA_DIR = "data_cache"
        if not os.path.exists(self.DATA_DIR):
            os.makedirs(self.DATA_DIR)
        
        # [ä¿®æ”¹ç‚¹1] å¢åŠ  Spot æ•°æ®ç¼“å­˜å˜é‡
        self.spot_data_cache = None
        self.spot_data_date = None

    def _standardize_dataframe(self, df, source_name):
        """æ ‡å‡†åŒ– DataFrameæ ¼å¼"""
        if df is None or df.empty:
            return df
        df = df.copy()
        for col in self.UNIFIED_COLUMNS:
            if col not in df.columns:
                df[col] = pd.NA
        df = df[self.UNIFIED_COLUMNS]
        # å¼ºåˆ¶è½¬ä¸ºæ•°å­—ç±»å‹ï¼Œé˜²æ­¢å‡ºç°å­—ç¬¦ä¸²
        numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 
                       'amplitude', 'pct_change', 'change', 'turnover_rate']
        for col in numeric_cols:
            if col in df.columns:
                df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')
        return df

    def _init_spot_data(self):
        """[æ–°å¢] ä»…åœ¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ï¼šæ‹‰å–å…¨å¸‚åœº ETF å®æ—¶è¡Œæƒ…"""
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # å¦‚æœç¼“å­˜é‡Œå·²ç»æœ‰ä»Šå¤©çš„æ•°æ®ï¼Œç›´æ¥è·³è¿‡
        if self.spot_data_cache is not None and self.spot_data_date == today_str:
            return True

        logger.info("ğŸš€ [ä¸œè´¢] æ­£åœ¨æ‹‰å–å…¨å¸‚åœº ETF å®æ—¶å¿«ç…§ (Spot)...")
        try:
            # è¿™é‡Œçš„æ¥å£éå¸¸å…³é”®ï¼Œè·å–æ‰€æœ‰ ETF çš„å½“å‰ä»·æ ¼
            df = ak.fund_etf_spot_em()
            
            if df is not None and not df.empty:
                # å»ºç«‹ä»£ç ç´¢å¼•ï¼Œæ–¹ä¾¿åç»­ O(1) å¤æ‚åº¦æŸ¥æ‰¾
                # æ³¨æ„ï¼šç¡®ä¿ä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
                df['code'] = df['ä»£ç '].astype(str)
                self.spot_data_cache = df.set_index('code')
                self.spot_data_date = today_str
                logger.info(f"âœ… å…¨å¸‚åœºå¿«ç…§è·å–æˆåŠŸï¼Œå…± {len(df)} æ¡æ•°æ®")
                return True
            else:
                logger.warning("âš ï¸ å…¨å¸‚åœºå¿«ç…§è¿”å›ä¸ºç©º")
        except Exception as e:
            logger.error(f"âŒ å…¨å¸‚åœºå¿«ç…§è·å–å¤±è´¥: {e}")
            self.spot_data_cache = None
        return False

    def _fetch_eastmoney(self, fund_code, fetch_time):
        """
        [é‡å†™] å³ä½¿æ˜¯è·å–ä¸œè´¢æ•°æ®ï¼Œä¹Ÿä¸å†è¯·æ±‚ç½‘ç»œï¼Œè€Œæ˜¯ä» spot ç¼“å­˜è¯»å– + æ‹¼æ¥æœ¬åœ°å†å²
        """
        # 1. ç¡®ä¿æœ‰å…¨é‡ç¼“å­˜
        if self.spot_data_cache is None:
            if not self._init_spot_data():
                return None, None # åˆå§‹åŒ–å¤±è´¥ï¼Œåç»­ä¼šè§¦å‘ failover å»è·‘æ–°æµª/è…¾è®¯

        # 2. åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾å½“å‰åŸºé‡‘
        if fund_code not in self.spot_data_cache.index:
            # è¿™ç§æƒ…å†µå¯èƒ½æ˜¯ä»£ç å¡«é”™äº†ï¼Œæˆ–è€…è¯¥åŸºé‡‘ä»Šæ—¥åœç‰Œ/æœªä¸Šå¸‚
            # logger.debug(f"âš ï¸ [Spot] æœªæ‰¾åˆ° {fund_code}")
            return None, None

        try:
            # 3. æå–å½“æ—¥æ•°æ®è¡Œ
            row = self.spot_data_cache.loc[fund_code]
            
            # æ„é€ å½“æ—¥çš„ DataFrame (å•è¡Œ)
            # æ³¨æ„ï¼šSpotæ¥å£æ²¡æœ‰å…·ä½“æ—¥æœŸå­—æ®µï¼Œé»˜è®¤å½’ä¸º"ä»Šå¤©"
            # å¿…é¡»ä½¿ç”¨ .date() ç¡®ä¿ç´¢å¼•å¯¹é½
            today_date = pd.Timestamp(datetime.now().date())
            
            new_data = {
                'date': today_date,
                'open': row['å¼€ç›˜ä»·'],
                'high': row['æœ€é«˜ä»·'],
                'low': row['æœ€ä½ä»·'],
                'close': row['æœ€æ–°ä»·'],
                'volume': row['æˆäº¤é‡'],
                'amount': row['æˆäº¤é¢'],
                'pct_change': row['æ¶¨è·Œå¹…'],
                'change': row.get('æ¶¨è·Œé¢', 0),
                'turnover_rate': row.get('æ¢æ‰‹ç‡', 0),
                'fetch_time': fetch_time,
                'source': 'eastmoney_spot'
            }
            
            df_new = pd.DataFrame([new_data])
            df_new.set_index('date', inplace=True)

            # 4. [æ ¸å¿ƒé€»è¾‘] è¯»å–æœ¬åœ° CSV å¹¶æ‹¼æ¥
            file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            
            if os.path.exists(file_path):
                try:
                    # è¯»å–æ—§æ•°æ®
                    df_old = pd.read_csv(file_path, index_col='date', parse_dates=['date'])
                    
                    # æ£€æŸ¥ä»Šå¤©çš„æ•°æ®æ˜¯å¦å·²å­˜åœ¨
                    if today_date in df_old.index:
                        # å¦‚æœå­˜åœ¨ï¼Œåˆ™æ›´æ–°è¿™ä¸€è¡Œï¼ˆè¦†ç›–ï¼‰
                        df_old.update(df_new)
                        df_final = df_old
                    else:
                        # å¦‚æœä¸å­˜åœ¨ï¼Œè¿½åŠ åˆ°æœ«å°¾
                        df_final = pd.concat([df_old, df_new])
                    
                    # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
                    df_final.sort_index(inplace=True)
                    return self._standardize_dataframe(df_final, "ä¸œè´¢"), "ä¸œè´¢"
                except Exception as e:
                    logger.error(f"âš ï¸ è¯»å–æœ¬åœ°æ–‡ä»¶ {fund_code} å¤±è´¥: {e}ï¼Œå°†ä»…è¿”å›å½“æ—¥æ•°æ®")
                    return self._standardize_dataframe(df_new, "ä¸œè´¢"), "ä¸œè´¢"
            else:
                # å¦‚æœæ²¡æœ‰æœ¬åœ°æ–‡ä»¶ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œï¼‰ï¼Œåˆ™åªè¿”å›è¿™ä¸€è¡Œæ•°æ®
                # æ³¨æ„ï¼šè¿™æ„å‘³ç€ä½ çš„ CSV é‡Œåªæœ‰è¿™ä¸€å¤©çš„æ•°æ®
                return self._standardize_dataframe(df_new, "ä¸œè´¢"), "ä¸œè´¢"

        except Exception as e:
            logger.error(f"âŒ [ä¸œè´¢Spot] è§£ææ•°æ®å¼‚å¸¸: {e}")
            return None, None

    # --- æ–°æµªå’Œè…¾è®¯çš„é€»è¾‘ä¿æŒåŸæ ·ï¼Œä½œä¸ºå¤‡ç”¨å…œåº• ---
    @retry(retries=2, delay=15)
    def _fetch_sina(self, fund_code, fetch_time):
        logger.info(f"ğŸŒ [æ–°æµª] è·å– {fund_code}...")
        try:
            df = ak.fund_etf_hist_sina(symbol=fund_code)
            if df is not None and not df.empty:
                # ç®€å•å¤„ç†æ–°æµªæ•°æ®æ ¼å¼
                if 'date' in df.columns: 
                    df['date'] = pd.to_datetime(df['date'])
                    df.set_index('date', inplace=True)
                df['fetch_time'] = fetch_time
                return self._standardize_dataframe(df, "æ–°æµª"), "æ–°æµª"
        except Exception as e:
            logger.error(f"æ–°æµªå¤±è´¥: {e}")
        return None, None

    @retry(retries=2, delay=15)
    def _fetch_tencent(self, fund_code, fetch_time):
        logger.info(f"ğŸŒ [è…¾è®¯] è·å– {fund_code}...")
        try:
            prefix = 'sh' if fund_code.startswith('5') else ('sz' if fund_code.startswith('1') else '')
            df = ak.stock_zh_a_hist_tx(symbol=f"{prefix}{fund_code}", start_date="20250101", adjust="qfq")
            if df is not None and not df.empty:
                df.rename(columns={'æ—¥æœŸ':'date', 'å¼€ç›˜':'open', 'æ”¶ç›˜':'close', 'æœ€é«˜':'high', 'æœ€ä½':'low', 'æˆäº¤é‡':'volume'}, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                df['fetch_time'] = fetch_time
                return self._standardize_dataframe(df, "è…¾è®¯"), "è…¾è®¯"
        except Exception as e:
            logger.error(f"è…¾è®¯å¤±è´¥: {e}")
        return None, None

    def _fetch_from_network(self, fund_code):
        """ä¸»è·å–é€»è¾‘"""
        fetch_time = get_beijing_time().strftime("%Y-%m-%d %H:%M:%S")

        # 1. ä¼˜å…ˆå°è¯•ä¸œè´¢ (ç°åœ¨æ˜¯æé€Ÿ Spot æ¨¡å¼)
        # ä¸éœ€è¦ sleep äº†ï¼Œå› ä¸ºæ˜¯è¯»å†…å­˜
        df, source = self._fetch_eastmoney(fund_code, fetch_time)
        if df is not None:
            return df, source

        # 2. å¦‚æœ Spot é‡Œæ²¡æœ‰ï¼ˆæ¯”å¦‚åœç‰Œï¼‰ï¼Œå°è¯•æ–°æµªï¼ˆè·å–å†å²ï¼‰
        time.sleep(random.uniform(2, 5))
        df, source = self._fetch_sina(fund_code, fetch_time)
        if df is not None:
            return df, source

        # 3. æœ€åå°è¯•è…¾è®¯
        time.sleep(random.uniform(2, 5))
        df, source = self._fetch_tencent(fund_code, fetch_time)
        if df is not None:
            return df, source
            
        return None, None

    def update_cache(self, fund_code):
        """æ›´æ–°æ¥å£ï¼Œä¿æŒå†™å…¥é€»è¾‘ä¸å˜"""
        df, source = self._fetch_from_network(fund_code)
        
        if df is not None and not df.empty:
            file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            # æ— è®ºæ¥æºæ˜¯å“ªï¼Œéƒ½ç›´æ¥è¦†ç›–å†™å…¥ï¼ˆå› ä¸º _fetch_eastmoney å·²ç»åšå¥½äº†æ‹¼æ¥ï¼‰
            df.to_csv(file_path)
            logger.info(f"ğŸ’¾ [{source}] {fund_code} æ•°æ®å·²æ›´æ–°")
            return True
        else:
            logger.error(f"âŒ {fund_code} æ›´æ–°å¤±è´¥")
            return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    print("ğŸš€ [DataFetcher] å¯åŠ¨ (Spot æé€Ÿæ¨¡å¼)...")
    
    # è¯»å–é…ç½®
    try:
        with open('config.yaml', 'r', encoding='utf-8') as f:
            cfg = yaml.safe_load(f)
            funds = cfg.get('funds', [])
    except:
        funds = [] # æ­¤æ—¶è¯·ç¡®ä¿ä½ çš„config.yamlå­˜åœ¨
    
    if not funds:
        print("âš ï¸ æœªæ‰¾åˆ°åŸºé‡‘åˆ—è¡¨")
        exit()

    fetcher = DataFetcher()
    
    # [å…³é”®æ­¥éª¤] åˆå§‹åŒ–å…¨å¸‚åœºæ•°æ® (åªè¯·æ±‚1æ¬¡)
    fetcher._init_spot_data()

    success_count = 0
    for idx, fund in enumerate(funds):
        code = str(fund.get('code')) # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
        name = fund.get('name')
        
        # è¿™é‡Œçš„ update_cache é€Ÿåº¦ä¼šéå¸¸å¿«
        if fetcher.update_cache(code):
            success_count += 1
            
        # æé€Ÿæ¨¡å¼ä¸‹ï¼Œä¸éœ€è¦ sleep å¾ˆä¹…ï¼Œå¾®å°çš„é—´éš”å³å¯
        if idx % 10 == 0: 
            print(f"è¿›åº¦: {idx+1}/{len(funds)}...")
            
    print(f"ğŸ å®Œæˆ: {success_count}/{len(funds)}")
