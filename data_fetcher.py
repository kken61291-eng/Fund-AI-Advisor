import akshare as ak
import pandas as pd
import time
import random
import os
import yaml
import logging
import requests
import gc
from datetime import datetime, time as dt_time

# ===================== å·¥å…·å‡½æ•° =====================
def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´ï¼ˆä¸œå…«åŒºï¼‰"""
    from datetime import timezone, timedelta
    return datetime.now(timezone(timedelta(hours=8)))

# ç®€æ˜“æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def retry(retries=3, delay=10):
    """ç®€æ˜“é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.warning(f"âš ï¸ [Retry {i+1}/{retries}] æ“ä½œå¤±è´¥: {e}, ç­‰å¾… {delay}s åé‡è¯•...")
                    if i == retries - 1:
                        logger.error(f"âŒ é‡è¯•è€—å°½ï¼Œæœ€ç»ˆå¤±è´¥: {e}")
                        return None, None 
                    time.sleep(delay)
            return None
        return wrapper
    return decorator

def force_close_connections():
    """[V17.0] å¼ºåˆ¶å…³é—­æ‰€æœ‰ç½‘ç»œè¿æ¥"""
    try:
        if hasattr(ak, '_session') and ak._session:
            try:
                ak._session.close()
                ak._session = None
            except:
                pass
        gc.collect()
        time.sleep(0.5)
    except Exception as e:
        logger.debug(f"å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")

# [æ–°å¢ V17.0] ä½¿ç”¨ curl_cffi åˆ›å»ºæ¨¡æ‹Ÿæµè§ˆå™¨ä¼šè¯
def create_browser_session():
    """åˆ›å»ºæ¨¡æ‹Ÿ Chrome æµè§ˆå™¨çš„ curl_cffi ä¼šè¯ï¼Œç»•è¿‡ TLS æŒ‡çº¹æ£€æµ‹"""
    try:
        from curl_cffi import requests as curl_requests
        
        # æ¨¡æ‹Ÿ Chrome 120 çš„ TLS æŒ‡çº¹
        session = curl_requests.Session(
            impersonate="chrome120",  # å…³é”®ï¼šæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨æŒ‡çº¹
            timeout=30
        )
        return session
    except ImportError:
        logger.warning("curl_cffi æœªå®‰è£…ï¼Œå›é€€åˆ°æ™®é€š requests")
        return None
# ====================================================================

class DataFetcher:
    UNIFIED_COLUMNS = [
        'date', 'open', 'high', 'low', 'close', 'volume',
        'amount', 'amplitude', 'pct_change', 'change', 'turnover_rate',
        'fetch_time'
    ]
    
    def __init__(self):
        self.DATA_DIR = "data_cache"
        if not os.path.exists(self.DATA_DIR):
            os.makedirs(self.DATA_DIR)
            
        # [V17.0] æ‰©å…… User-Agent æ± ï¼Œå¢åŠ ç§»åŠ¨ç«¯
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (iPad; CPU OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
        ]

    def _get_random_headers(self):
        """ç”Ÿæˆéšæœºè¯·æ±‚å¤´"""
        ua = random.choice(self.user_agents)
        return {
            'User-Agent': ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-US;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'close',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }

    def _verify_data_freshness(self, df, fund_code, source_name):
        """æ•°æ®æ–°é²œåº¦å®¡è®¡"""
        if df is None or df.empty: 
            return
        
        try:
            last_date = pd.to_datetime(df.index[-1]).date()
            now_bj = get_beijing_time()
            today_date = now_bj.date()
            is_trading_time = (dt_time(9, 30) <= now_bj.time() <= dt_time(15, 0))
            
            log_prefix = f"ğŸ“… [{source_name}] {fund_code} æœ€æ–°æ—¥æœŸ: {last_date}"
            
            if last_date == today_date:
                logger.info(f"{log_prefix} | âœ… æ•°æ®å·²æ›´æ–°è‡³ä»Šæ—¥")
            elif last_date < today_date:
                days_gap = (today_date - last_date).days
                if is_trading_time and days_gap >= 1:
                    logger.warning(f"{log_prefix} | âš ï¸ æ•°æ®æ»å {days_gap} å¤©")
                else:
                    logger.info(f"{log_prefix} | â¸ï¸ å†å²æ•°æ®å°±ç»ª")
        except Exception as e:
            logger.warning(f"å®¡è®¡æ•°æ®æ–°é²œåº¦å¤±è´¥: {e}")

    def _standardize_dataframe(self, df, source_name):
        """æ ‡å‡†åŒ– DataFrame"""
        if df is None or df.empty:
            return df
        
        df = df.copy()
            
        for col in self.UNIFIED_COLUMNS:
            if col not in df.columns:
                df[col] = pd.NA
        
        df = df[self.UNIFIED_COLUMNS]
        
        numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 
                       'amplitude', 'pct_change', 'change', 'turnover_rate']
        for col in numeric_cols:
            if col in df.columns:
                df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')
        
        return df

    @retry(retries=2, delay=25)
    def _fetch_eastmoney(self, fund_code, fetch_time):
        """[V17.0] ä½¿ç”¨ curl_cffi æ¨¡æ‹Ÿæµè§ˆå™¨è·å–ä¸œè´¢æ•°æ®"""
        logger.info(f"ğŸŒ [ä¸œè´¢] æ¨¡æ‹Ÿæµè§ˆå™¨è·å– {fund_code}...")
        
        try:
            # [å…³é”® V17.0] ä½¿ç”¨ curl_cffi çš„æµè§ˆå™¨æ¨¡æ‹ŸåŠŸèƒ½
            # è¿™ä¼šè‡ªåŠ¨å¤„ç† TLS æŒ‡çº¹ã€HTTP/2 ç­‰
            browser_session = create_browser_session()
            
            if browser_session:
                # ä½¿ç”¨ curl_cffi æ—¶ï¼Œé€šè¿‡ akshare çš„åº•å±‚æœºåˆ¶æ³¨å…¥
                # æ³¨æ„ï¼šakshare 1.18+ å†…éƒ¨ä½¿ç”¨äº† curl_cffiï¼Œæˆ‘ä»¬å°è¯•è®¾ç½®å…¶ session
                try:
                    # å°è¯•æ›¿æ¢ akshare å†…éƒ¨ session
                    original_session = getattr(ak, '_session', None)
                    ak._session = browser_session
                except:
                    browser_session = None
            
            # è°ƒç”¨æ¥å£
            df = ak.fund_etf_hist_em(
                symbol=fund_code, 
                period="daily", 
                start_date="20250101", 
                end_date="20500101", 
                adjust="qfq"
            )
            
            # æ¢å¤åŸå§‹ session
            try:
                if browser_session and original_session:
                    ak._session = original_session
            except:
                pass
            
            if df is not None and not df.empty:
                rename_map = {
                    'æ—¥æœŸ': 'date',
                    'å¼€ç›˜': 'open',
                    'æ”¶ç›˜': 'close',
                    'æœ€é«˜': 'high',
                    'æœ€ä½': 'low',
                    'æˆäº¤é‡': 'volume',
                    'æˆäº¤é¢': 'amount',
                    'æŒ¯å¹…': 'amplitude',
                    'æ¶¨è·Œå¹…': 'pct_change',
                    'æ¶¨è·Œé¢': 'change',
                    'æ¢æ‰‹ç‡': 'turnover_rate'
                }
                df.rename(columns=rename_map, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                df['fetch_time'] = fetch_time
                df['source'] = 'eastmoney'
                
                df = self._standardize_dataframe(df, "ä¸œè´¢")
                return df, "ä¸œè´¢"
                
        finally:
            force_close_connections()
            logger.info(f"ğŸ”Œ [ä¸œè´¢] ä¼šè¯å·²æ¸…ç†")

    @retry(retries=2, delay=15)
    def _fetch_sina(self, fund_code, fetch_time):
        """[V17.0] è·å–æ–°æµªæ•°æ®"""
        logger.info(f"ğŸŒ [æ–°æµª] è·å– {fund_code}...")
        
        try:
            df = ak.fund_etf_hist_sina(symbol=fund_code)
            
            if df is not None and not df.empty:
                if df.index.name in ['date', 'æ—¥æœŸ']: 
                    df = df.reset_index()
                
                col_mapping = {}
                for col in df.columns:
                    col_str = str(col).lower()
                    if col_str in ['date', 'æ—¥æœŸ']:
                        col_mapping[col] = 'date'
                    elif col_str in ['open', 'å¼€ç›˜']:
                        col_mapping[col] = 'open'
                    elif col_str in ['close', 'æ”¶ç›˜', 'latest']:
                        col_mapping[col] = 'close'
                    elif col_str in ['high', 'æœ€é«˜']:
                        col_mapping[col] = 'high'
                    elif col_str in ['low', 'æœ€ä½']:
                        col_mapping[col] = 'low'
                    elif col_str in ['volume', 'æˆäº¤é‡', 'vol']:
                        col_mapping[col] = 'volume'
                
                df.rename(columns=col_mapping, inplace=True)
                
                if 'date' in df.columns:
                    df['date'] = pd.to_datetime(df['date'])
                    df.set_index('date', inplace=True)
                    
                    df['amount'] = pd.NA
                    df['amplitude'] = pd.NA
                    df['pct_change'] = pd.NA
                    df['change'] = pd.NA
                    df['turnover_rate'] = pd.NA
                    df['fetch_time'] = fetch_time
                    df['source'] = 'sina'
                    
                    for col in ['open', 'high', 'low', 'close', 'volume']:
                        if col in df.columns: 
                            df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')
                    
                    df = self._standardize_dataframe(df, "æ–°æµª")
                    return df, "æ–°æµª"
        finally:
            force_close_connections()
            logger.info(f"ğŸ”Œ [æ–°æµª] è¿æ¥å·²å…³é—­")

    @retry(retries=2, delay=15)
    def _fetch_tencent(self, fund_code, fetch_time):
        """[V17.0] è·å–è…¾è®¯æ•°æ®"""
        logger.info(f"ğŸŒ [è…¾è®¯] è·å– {fund_code}...")
        
        try:
            prefix = 'sh' if fund_code.startswith('5') else ('sz' if fund_code.startswith('1') else '')
            if prefix:
                df = ak.stock_zh_a_hist_tx(
                    symbol=f"{prefix}{fund_code}", 
                    start_date="20200101", 
                    adjust="qfq"
                )
                
                if df is not None and not df.empty:
                    rename_map = {
                        'æ—¥æœŸ': 'date',
                        'å¼€ç›˜': 'open',
                        'æ”¶ç›˜': 'close',
                        'æœ€é«˜': 'high',
                        'æœ€ä½': 'low',
                        'æˆäº¤é‡': 'volume',
                        'æˆäº¤é¢': 'amount',
                        'æŒ¯å¹…': 'amplitude',
                        'æ¶¨è·Œå¹…': 'pct_change',
                        'æ¶¨è·Œé¢': 'change',
                        'æ¢æ‰‹ç‡': 'turnover_rate'
                    }
                    df.rename(columns=rename_map, inplace=True)
                    df['date'] = pd.to_datetime(df['date'])
                    df.set_index('date', inplace=True)
                    df['fetch_time'] = fetch_time
                    df['source'] = 'tencent'
                    
                    df = self._standardize_dataframe(df, "è…¾è®¯")
                    return df, "è…¾è®¯"
        finally:
            force_close_connections()
            logger.info(f"ğŸ”Œ [è…¾è®¯] è¿æ¥å·²å…³é—­")

    def _fetch_from_network(self, fund_code):
        """[V17.0] ä¸»è·å–é€»è¾‘ï¼šä¸œè´¢ -> æ–°æµª -> è…¾è®¯"""
        fetch_time = get_beijing_time().strftime("%Y-%m-%d %H:%M:%S")
        
        # 1. ä¸œè´¢ - ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿ
        try:
            wait = random.uniform(8.0, 15.0)  # [V17.0] å¢åŠ åˆå§‹ç­‰å¾…
            logger.info(f"â³ é¢„ç­‰å¾… {wait:.1f}s...")
            time.sleep(wait)
            
            df, source = self._fetch_eastmoney(fund_code, fetch_time)
            if df is not None and not df.empty:
                return df, source
        except Exception as e:
            logger.error(f"âŒ ä¸œè´¢å¤±è´¥: {e}")
            force_close_connections()

        # 2. æ–°æµª
        try:
            time.sleep(random.uniform(5.0, 10.0))
            df, source = self._fetch_sina(fund_code, fetch_time)
            if df is not None and not df.empty:
                return df, source
        except Exception as e:
            logger.error(f"âš ï¸ æ–°æµªå¤±è´¥: {e}")

        # 3. è…¾è®¯
        try:
            time.sleep(random.uniform(5.0, 10.0))
            df, source = self._fetch_tencent(fund_code, fetch_time)
            if df is not None and not df.empty:
                return df, source
        except Exception as e:
            logger.error(f"âš ï¸ è…¾è®¯å¤±è´¥: {e}")
        
        return None, None

    def update_cache(self, fund_code):
        """[V17.0] æ›´æ–°å•ä¸ªåŸºé‡‘æ•°æ®"""
        df, source = self._fetch_from_network(fund_code)
        
        if df is None:
            logger.error(f"âŒ {fund_code} æ‰€æœ‰æ•°æ®æºå‡è·å–å¤±è´¥")
            return False

        if not df.empty:
            file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            df.to_csv(file_path)
            logger.info(f"ğŸ’¾ [{source}] {fund_code} æ•°æ®å·²ä¿å­˜è‡³ {file_path}")
            
            # [V17.0] ä¸œè´¢æˆåŠŸåç­‰å¾… 50-70 ç§’ï¼ˆæ›´ä¿å®ˆï¼‰
            if source == "ä¸œè´¢":
                wait_time = random.uniform(50, 70)
                logger.info(f"â³ [ä¸œè´¢] å¼ºåˆ¶å†·å´ {wait_time:.1f}s...")
                time.sleep(wait_time)
            
            return True
        else:
            logger.error(f"âŒ {fund_code} æ•°æ®ä¸ºç©º")
            return False

    def get_fund_history(self, fund_code, days=250):
        """è¯»å–æœ¬åœ°ç¼“å­˜"""
        file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
        
        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ æœ¬åœ°ç¼“å­˜ç¼ºå¤±: {fund_code}")
            return None
            
        try:
            df = pd.read_csv(file_path, index_col='date', parse_dates=['date'])
            
            if 'fetch_time' in df.columns:
                df['fetch_time'] = pd.to_datetime(df['fetch_time'])
            
            self._verify_data_freshness(df, fund_code, "æœ¬åœ°ç¼“å­˜")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–æœ¬åœ°ç¼“å­˜å¤±è´¥ {fund_code}: {e}")
            return None

# ==========================================
# [V17.0] ä¸»ç¨‹åºå…¥å£ - éšæœºé¡ºåº + æµè§ˆå™¨æ¨¡æ‹Ÿ
# ==========================================
if __name__ == "__main__":
    print("ğŸš€ [DataFetcher] å¯åŠ¨ (V17.0 Browser-Impersonate Mode)...")
    
    def load_config_local():
        try:
            with open('config.yaml', 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except:
            return {}

    cfg = load_config_local()
    funds = cfg.get('funds', [])
    
    if not funds:
        print("âš ï¸ æœªæ‰¾åˆ°åŸºé‡‘åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ config.yaml")
        exit()

    # éšæœºæ‰“ä¹±è·å–é¡ºåº
    random.shuffle(funds)
    logger.info(f"ğŸ² éšæœºè·å–é¡ºåº: {[f.get('code') for f in funds]}")

    fetcher = DataFetcher()
    success_count = 0
    
    for idx, fund in enumerate(funds):
        code = fund.get('code')
        name = fund.get('name')
        print(f"ğŸ”„ [{idx+1}/{len(funds)}] æ›´æ–°: {name} ({code})...")
        
        try:
            if fetcher.update_cache(code):
                success_count += 1
            
            # åŸºé‡‘é—´åŸºç¡€é—´éš”
            if idx < len(funds) - 1:
                base_wait = random.uniform(5.0, 10.0)
                logger.info(f"â³ åŸºç¡€é—´éš”ç­‰å¾… {base_wait:.1f}s...")
                time.sleep(base_wait)
                
        except Exception as e:
            print(f"âŒ æ›´æ–°å¼‚å¸¸ {name}: {e}")
            force_close_connections()
            time.sleep(random.uniform(15, 20))
            
    print(f"ğŸ å®Œæˆ: {success_count}/{len(funds)} (æµè§ˆå™¨æ¨¡æ‹Ÿæ¨¡å¼)")
